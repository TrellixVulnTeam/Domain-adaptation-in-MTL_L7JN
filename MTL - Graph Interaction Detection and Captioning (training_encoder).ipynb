{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import sys\n",
    "import random\n",
    "\n",
    "import json\n",
    "import h5py\n",
    "import itertools\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import argparse, pickle\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from torch import optim\n",
    "import torchvision.models\n",
    "from torch.utils.data import Dataset as torchDataset\n",
    "from torch.utils.data import DataLoader as TorchDataLoader\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "\n",
    "# caption libraries\n",
    "import evaluation\n",
    "import collections\n",
    "from data.example import Example\n",
    "from data.utils import nostdout\n",
    "from data.field import ImageDetectionsField, TextField, RawField\n",
    "from models.transformer import Transformer, MemoryAugmentedEncoder, MeshedDecoder, ScaledDotProductAttentionMemory\n",
    "\n",
    "# graph libraries\n",
    "import utils.io as io\n",
    "import matplotlib.pyplot as plt\n",
    "from utils.g_vis_img import *\n",
    "from models.graph_su import *\n",
    "from evaluation.graph_eval import *\n",
    "\n",
    "# feature extractor\n",
    "from models.feature_extractor import *\n",
    "\n",
    "# Random seeds\n",
    "seed = 27\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SurgicalSceneConstants():\n",
    "    '''\n",
    "    Surgical Scene constants\n",
    "    '''\n",
    "    def __init__( self):\n",
    "        self.instrument_classes = ( 'kidney', 'bipolar_forceps', 'prograsp_forceps', 'large_needle_driver',\n",
    "                                'monopolar_curved_scissors', 'ultrasound_probe', 'suction', 'clip_applier',\n",
    "                                'stapler', 'maryland_dissector', 'spatulated_monopolar_cautery')\n",
    "        self.action_classes = ( 'Idle', 'Grasping', 'Retraction', 'Tissue_Manipulation', \n",
    "                                'Tool_Manipulation', 'Cutting', 'Cauterization', \n",
    "                                'Suction', 'Looping', 'Suturing', 'Clipping', 'Staple', \n",
    "                                'Ultrasound_Sensing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-entropy loss with label smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CELossWithLS(torch.nn.Module):\n",
    "    '''\n",
    "    label smoothing cross-entropy loss for captioning\n",
    "    '''\n",
    "    def __init__(self, classes=None, smoothing=0.1, gamma=3.0, isCos=True, ignore_index=-1):\n",
    "        super(CELossWithLS, self).__init__()\n",
    "        self.complement = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.cls = classes\n",
    "        self.log_softmax = torch.nn.LogSoftmax(dim=1)\n",
    "        self.gamma = gamma\n",
    "        self.ignore_index = ignore_index\n",
    "\n",
    "    def forward(self, logits, target):\n",
    "        with torch.no_grad():\n",
    "            oh_labels = F.one_hot(target.to(torch.int64), num_classes = self.cls).permute(0,1,2).contiguous()\n",
    "            smoothen_ohlabel = oh_labels * self.complement + self.smoothing / self.cls\n",
    "\n",
    "        logs = self.log_softmax(logits[target!=self.ignore_index])\n",
    "        pt = torch.exp(logs)\n",
    "        return -torch.sum((1-pt).pow(self.gamma)*logs * smoothen_ohlabel[target!=self.ignore_index], dim=1).mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader(TorchDataLoader):\n",
    "    def __init__(self, dataset, *args, **kwargs):\n",
    "        super(DataLoader, self).__init__(dataset, *args, collate_fn=dataset.collate_fn(), **kwargs)\n",
    "\n",
    "class Dataset(object):\n",
    "    def __init__(self, examples, fields, gsu_const):\n",
    "        self.examples = examples\n",
    "        self.fields = dict(fields)\n",
    "        \n",
    "        self.file_dir = gsu_const['file_dir']\n",
    "        self.img_dir = gsu_const['img_dir']\n",
    "        self.dataconst = gsu_const['dataconst']\n",
    "        self.feature_extractor = gsu_const['feature_extractor']\n",
    "        self.word2vec = h5py.File(gsu_const['w2v_loc'], 'r')\n",
    "        \n",
    "    # word2vec\n",
    "    def _get_word2vec(self,node_ids):\n",
    "        word2vec = np.empty((0,300))\n",
    "        for node_id in node_ids:\n",
    "            vec = self.word2vec[self.dataconst.instrument_classes[node_id]]\n",
    "            word2vec = np.vstack((word2vec, vec))\n",
    "        return word2vec\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        example = self.examples[i]\n",
    "        frame_path = getattr(example, 'image')\n",
    "        frame_path = frame_path.split(\"/\")\n",
    "        \n",
    "        _img_loc = os.path.join(self.file_dir, frame_path[0],self.img_dir,frame_path[3].split(\"_\")[0]+'.png')\n",
    "        frame_data = h5py.File(os.path.join(self.file_dir, frame_path[0],'vsgat',self.feature_extractor, frame_path[3].split(\"_\")[0]+'_features.hdf5'), 'r')    \n",
    "        \n",
    "        #print(_img_loc)\n",
    "        \n",
    "        # caption data\n",
    "        cp_data = []\n",
    "        for field_name, field in self.fields.items():\n",
    "            if field_name == 'image' and field == None:\n",
    "                cp_data.append(np.zeros((6,512), dtype = np.float32))\n",
    "            else:\n",
    "                cp_data.append(field.preprocess(getattr(example, field_name)))   \n",
    "        if len(cp_data) == 1: cp_data = cp_data[0]\n",
    "        \n",
    "        # graph data\n",
    "        gsu_data = {}\n",
    "        gsu_data['img_name'] = frame_data['img_name'].value[:] + '.jpg'\n",
    "        gsu_data['img_loc'] = _img_loc\n",
    "        gsu_data['node_num'] = frame_data['node_num'].value\n",
    "        gsu_data['roi_labels'] = frame_data['classes'][:]\n",
    "        gsu_data['det_boxes'] = frame_data['boxes'][:]\n",
    "        gsu_data['edge_labels'] = frame_data['edge_labels'][:]\n",
    "        gsu_data['edge_num'] = gsu_data['edge_labels'].shape[0]\n",
    "        if self.fields['image'] == None:\n",
    "            gsu_data['features'] = np.zeros((gsu_data['node_num'],512), dtype = np.float32)\n",
    "        else:\n",
    "            gsu_data['features'] = frame_data['node_features'][:]\n",
    "        gsu_data['spatial_feat'] = frame_data['spatial_features'][:]\n",
    "        gsu_data['word2vec'] = self._get_word2vec(gsu_data['roi_labels'])\n",
    "        \n",
    "        data = {}\n",
    "        data['cp_data'] = cp_data\n",
    "        data['gsu_data'] = gsu_data\n",
    "        return data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getattr__(self, attr):\n",
    "        if attr in self.fields:\n",
    "            for x in self.examples:\n",
    "                yield getattr(x, attr)\n",
    "                \n",
    "    def collate_fn(self):\n",
    "        def collate(batch):\n",
    "            gsu_batch_data = {}\n",
    "            gsu_batch_data['img_name'] = []\n",
    "            gsu_batch_data['img_loc'] = []\n",
    "            gsu_batch_data['node_num'] = []\n",
    "            gsu_batch_data['roi_labels'] = []\n",
    "            gsu_batch_data['det_boxes'] = []\n",
    "            gsu_batch_data['edge_labels'] = []\n",
    "            gsu_batch_data['edge_num'] = []\n",
    "            gsu_batch_data['features'] = []\n",
    "            gsu_batch_data['spatial_feat'] = []\n",
    "            gsu_batch_data['word2vec'] = []\n",
    "\n",
    "            for data in batch:\n",
    "                gsu_batch_data['img_name'].append(data['gsu_data']['img_name'])\n",
    "                gsu_batch_data['img_loc'].append(data['gsu_data']['img_loc'])\n",
    "                gsu_batch_data['node_num'].append(data['gsu_data']['node_num'])\n",
    "                gsu_batch_data['roi_labels'].append(data['gsu_data']['roi_labels'])\n",
    "                gsu_batch_data['det_boxes'].append(data['gsu_data']['det_boxes'])\n",
    "                gsu_batch_data['edge_labels'].append(data['gsu_data']['edge_labels'])\n",
    "                gsu_batch_data['edge_num'].append(data['gsu_data']['edge_num'])\n",
    "                gsu_batch_data['features'].append(data['gsu_data']['features'])\n",
    "                gsu_batch_data['spatial_feat'].append(data['gsu_data']['spatial_feat'])\n",
    "                gsu_batch_data['word2vec'].append(data['gsu_data']['word2vec'])\n",
    "\n",
    "            gsu_batch_data['edge_labels'] = torch.FloatTensor(np.concatenate(gsu_batch_data['edge_labels'], axis=0))\n",
    "            gsu_batch_data['features'] = torch.FloatTensor(np.concatenate(gsu_batch_data['features'], axis=0))\n",
    "            gsu_batch_data['spatial_feat'] = torch.FloatTensor(np.concatenate(gsu_batch_data['spatial_feat'], axis=0))\n",
    "            gsu_batch_data['word2vec'] = torch.FloatTensor(np.concatenate(gsu_batch_data['word2vec'], axis=0))\n",
    "            \n",
    "            cp_batch_data = []\n",
    "            tensors = []\n",
    "            \n",
    "            for data in batch: cp_batch_data.append(data['cp_data'])\n",
    "            if len(self.fields) == 1: cp_batch_data = [cp_batch_data, ]\n",
    "            else: cp_batch_data = list(zip(*cp_batch_data))\n",
    "\n",
    "            for field, data in zip(self.fields.values(), cp_batch_data):\n",
    "                if field == None: tensor = default_collate(data)\n",
    "                else: tensor = field.process(data)\n",
    "                if isinstance(tensor, collections.Sequence) and any(isinstance(t, torch.Tensor) for t in tensor):\n",
    "                    tensors.extend(tensor)\n",
    "                else: tensors.append(tensor)\n",
    "\n",
    "            if len(tensors) > 1:cp_batch_data = tensors\n",
    "            else: cp_batch_data = tensors[0]\n",
    "            \n",
    "            batch_data = {}\n",
    "            batch_data['gsu'] = gsu_batch_data\n",
    "            batch_data['cp'] = cp_batch_data\n",
    "            \n",
    "            return(batch_data)\n",
    "\n",
    "        return collate\n",
    "\n",
    "\n",
    "class PairedDataset(Dataset):\n",
    "    def __init__(self, examples, fields, gsu_const):\n",
    "        assert ('image' in fields)\n",
    "        assert ('text' in fields)\n",
    "        super(PairedDataset, self).__init__(examples, fields, gsu_const)\n",
    "        self.image_field = self.fields['image']\n",
    "        if self.image_field == None: print('no pre-extracted image featured')\n",
    "        self.text_field = self.fields['text']\n",
    "        \n",
    "    def image_dictionary(self, fields=None):\n",
    "        if not fields:\n",
    "            fields = self.fields\n",
    "        dataset = Dataset(self.examples, fields, gsu_const)\n",
    "        #dataset = DictionaryDataset(self.examples, fields, gsu_const, 'image')\n",
    "        return dataset\n",
    "        \n",
    "class COCO(PairedDataset):\n",
    "    def __init__(self, image_field, text_field, gsu_const, img_root, ann_root, id_root=None):\n",
    "        # setting training and val root\n",
    "        roots = {}\n",
    "        roots['train'] = { 'img': img_root, 'cap': os.path.join(ann_root, 'captions_train.json')}\n",
    "        roots['val'] = {'img': img_root, 'cap': os.path.join(ann_root, 'captions_val.json')}\n",
    "\n",
    "        # Getting the id: planning to remove this in future\n",
    "        if id_root is not None:\n",
    "            ids = {}\n",
    "            ids['train'] = json.load(open(os.path.join(id_root, 'WithCaption_id_path_train.json'), 'r'))\n",
    "            ids['val'] = json.load(open(os.path.join(id_root, 'WithCaption_id_path_val.json'), 'r'))   \n",
    "        else: ids = None\n",
    "        \n",
    "        with nostdout():\n",
    "            self.train_examples, self.val_examples = self.get_samples(roots, ids)\n",
    "        examples = self.train_examples + self.val_examples\n",
    "        super(COCO, self).__init__(examples, {'image': image_field, 'text': text_field}, gsu_const)   \n",
    "\n",
    "    @property\n",
    "    def splits(self):\n",
    "        train_split = PairedDataset(self.train_examples, self.fields, gsu_const) \n",
    "        val_split = PairedDataset(self.val_examples, self.fields, gsu_const)\n",
    "        return train_split, val_split\n",
    "\n",
    "    @classmethod\n",
    "    def get_samples(cls, roots, ids_dataset=None):\n",
    "        train_samples = []\n",
    "        val_samples = []\n",
    "   \n",
    "        for split in ['train', 'val']:\n",
    "            anns = json.load(open(roots[split]['cap'], 'r'))\n",
    "            if ids_dataset is not None: ids = ids_dataset[split]\n",
    "                \n",
    "            for index in range(len(ids)):              \n",
    "                id_path = ids[index]\n",
    "                caption = anns[index]['caption']\n",
    "                example = Example.fromdict({'image': os.path.join('', id_path), 'text': caption})\n",
    "                if split == 'train': train_samples.append(example)\n",
    "                elif split == 'val': val_samples.append(example)\n",
    "                    \n",
    "        return train_samples, val_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MTL Model (Graph Scene Understanding and Captioning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mtl_model(nn.Module):\n",
    "    '''\n",
    "    Multi-task model : Graph Scene Understanding and Captioning\n",
    "    '''\n",
    "    def __init__(self, feature_extractor, graph, caption):\n",
    "        super(mtl_model, self).__init__()\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.graph_su = graph\n",
    "        self.caption = caption\n",
    "        self.transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
    "    \n",
    "    def forward(self, img_dir, det_boxes_all, caps_gt, node_num, features, spatial_feat, word2vec, roi_labels, val = False, text_field = None):               \n",
    "        \n",
    "        gsu_node_feat = None\n",
    "        cp_node_feat = None\n",
    "        for index, img_loc in  enumerate(img_dir):\n",
    "            #print(img_loc)\n",
    "            _img = Image.open(img_loc).convert('RGB')\n",
    "            _img = np.array(_img)\n",
    "            \n",
    "            img_stack = None\n",
    "            for idx, bndbox in enumerate(det_boxes_all[index]):        \n",
    "                roi = np.array(bndbox).astype(int)\n",
    "                roi_image = _img[roi[1]:roi[3] + 1, roi[0]:roi[2] + 1, :]\n",
    "                roi_image = self.transform(cv2.resize(roi_image, (224, 224), interpolation=cv2.INTER_LINEAR))\n",
    "                roi_image = torch.autograd.Variable(roi_image.unsqueeze(0))\n",
    "                # stack nodes images per image\n",
    "                if img_stack is None: \n",
    "                    img_stack = roi_image\n",
    "                else: img_stack = torch.cat((img_stack, roi_image))\n",
    "            \n",
    "            #print(img_stack.shape)\n",
    "            # send the stack to feature extractor\n",
    "            img_stack = img_stack.cuda()\n",
    "            feature = feature_network(img_stack)\n",
    "            feature = feature.view(feature.size(0), -1)\n",
    "            \n",
    "            if gsu_node_feat == None: gsu_node_feat = feature\n",
    "            else: gsu_node_feat = torch.cat((gsu_node_feat,feature))\n",
    "            \n",
    "            feature = torch.unsqueeze(torch.cat((feature,torch.zeros((6-len(feature)),512).cuda())),0)\n",
    "            if cp_node_feat == None: cp_node_feat = feature\n",
    "            else: cp_node_feat = torch.cat((cp_node_feat,feature))\n",
    "\n",
    "    \n",
    "        if val == True:\n",
    "            caption_output, _ = self.caption.beam_search(cp_node_feat, 20, text_field.vocab.stoi['<eos>'], 5, out_size=1)\n",
    "        else:\n",
    "            caption_output = self.caption(cp_node_feat, caps_gt)\n",
    "        interaction = self.graph_su(node_num, gsu_node_feat, spatial_feat, word2vec, roi_labels, validation=val)\n",
    "        return interaction, caption_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def eval_mtl(model, dataloader, text_field):\n",
    "    \n",
    "    model.eval()\n",
    "    gen = {}\n",
    "    gts = {}\n",
    "\n",
    "    # graph\n",
    "    # criterion and scheduler\n",
    "    g_criterion = nn.MultiLabelSoftMarginLoss()                   \n",
    "    g_edge_count = 0\n",
    "    g_total_acc = 0.0\n",
    "    g_total_loss = 0.0\n",
    "    g_logits_list = []\n",
    "    g_labels_list = []\n",
    "    \n",
    "    for it, data in tqdm(enumerate(iter(dataloader))):\n",
    "            \n",
    "        graph_data = data['gsu']\n",
    "        cp_data = data['cp']\n",
    "            \n",
    "        # graph\n",
    "        img_name = graph_data['img_name']\n",
    "        img_loc = graph_data['img_loc']\n",
    "        node_num = graph_data['node_num']\n",
    "        roi_labels = graph_data['roi_labels']\n",
    "        det_boxes = graph_data['det_boxes']\n",
    "        edge_labels = graph_data['edge_labels']\n",
    "        edge_num = graph_data['edge_num']\n",
    "        features = graph_data['features']\n",
    "        spatial_feat = graph_data['spatial_feat']\n",
    "        word2vec = graph_data['word2vec']\n",
    "        features, spatial_feat, word2vec, edge_labels = features.to(device), spatial_feat.to(device), word2vec.to(device), edge_labels.to(device)         \n",
    "        \n",
    "        #print(features, features.shape)\n",
    "        # caption\n",
    "        _, caps_gt = cp_data\n",
    "            \n",
    "        with torch.no_grad():\n",
    "    \n",
    "            g_output, caption_out = model(img_loc, det_boxes, caps_gt, node_num, features, spatial_feat, word2vec, roi_labels, val = True, text_field = text_field)\n",
    "        \n",
    "            g_logits_list.append(g_output)\n",
    "            g_labels_list.append(edge_labels)\n",
    "            # loss and accuracy\n",
    "            g_loss = g_criterion(g_output, edge_labels.float())\n",
    "            g_acc = np.sum(np.equal(np.argmax(g_output.cpu().data.numpy(), axis=-1), np.argmax(edge_labels.cpu().data.numpy(), axis=-1)))\n",
    "            \n",
    "        # accumulate loss and accuracy of the batch\n",
    "        g_total_loss += g_loss.item() * edge_labels.shape[0]\n",
    "        g_total_acc  += g_acc\n",
    "        g_edge_count += edge_labels.shape[0]\n",
    "        \n",
    "        caps_gen = text_field.decode(caption_out, join_words=False)\n",
    "        \n",
    "        for i, (gts_i, gen_i) in enumerate(zip(caps_gt, caps_gen)):\n",
    "            gen_i = ' '.join([k for k, g in itertools.groupby(gen_i)])\n",
    "            gen['%d_%d' % (it, i)] = [gen_i, ]    \n",
    "            gts['%d_%d' % (it, i)] = [gts_i,]\n",
    "        \n",
    "    #graph evaluation\n",
    "    g_total_acc = g_total_acc / g_edge_count\n",
    "    g_total_loss = g_total_loss / len(dataloader)\n",
    "\n",
    "    g_logits_all = torch.cat(g_logits_list).cuda()\n",
    "    g_labels_all = torch.cat(g_labels_list).cuda()\n",
    "    g_logits_all = F.softmax(g_logits_all, dim=1)\n",
    "    g_map_value, g_ece, g_sce, g_tace, g_brier, g_uce = calibration_metrics(g_logits_all, g_labels_all, 'test')\n",
    "    \n",
    "    # caption evaluation\n",
    "    #if not os.path.exists('results/c_results/predict_caption'):\n",
    "    #    os.makedirs('results/c_results/predict_caption')\n",
    "    #json.dump(gen, open('results/c_results/predict_caption/predict_caption_val.json', 'w'))\n",
    "\n",
    "    gts = evaluation.PTBTokenizer.tokenize(gts)\n",
    "    gen = evaluation.PTBTokenizer.tokenize(gen)\n",
    "\n",
    "    scores, _ = evaluation.compute_scores(gts, gen)\n",
    "    print('Graph : {acc: %0.6f map: %0.6f loss: %0.6f, ece:%0.6f, sce:%0.6f, tace:%0.6f, brier:%.6f, uce:%.6f}' %(g_total_acc, g_map_value, g_total_loss, g_ece, g_sce, g_tace, g_brier, g_uce.item()) )\n",
    "    print(print(\"Caption Scores :\", scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train(epoch, lrc, model, dataloader, dict_dataloader_val, text_field):\n",
    "    '''\n",
    "    Finding optimal temperature scale for graph scene understanding task\n",
    "    '''\n",
    "    \n",
    "        \n",
    "    #if args.optim == 'sgd': \n",
    "    optimizer = optim.SGD(model.feature_extractor.parameters(), lr= lrc, momentum=0.9, weight_decay=0)\n",
    "    #else: \n",
    "    #    optimizer = optim.Adam(model.parameters(), lr= lrc, weight_decay=0)\n",
    "       \n",
    "    g_criterion = nn.MultiLabelSoftMarginLoss()\n",
    "    \n",
    "    for epoch_count in range(epoch):\n",
    "        \n",
    "        running_loss = 0.0\n",
    "        running_g_acc = 0.0\n",
    "        running_edge_count = 0\n",
    "        iters = 0\n",
    "        \n",
    "        for it, data in tqdm(enumerate(iter(dataloader))):\n",
    "            iters += 1\n",
    "            \n",
    "            graph_data = data['gsu']\n",
    "            cp_data = data['cp']\n",
    "            \n",
    "            # graph\n",
    "            img_name = graph_data['img_name']\n",
    "            img_loc = graph_data['img_loc']\n",
    "            node_num = graph_data['node_num']\n",
    "            roi_labels = graph_data['roi_labels']\n",
    "            det_boxes = graph_data['det_boxes']\n",
    "            edge_labels = graph_data['edge_labels']\n",
    "            edge_num = graph_data['edge_num']\n",
    "            features = graph_data['features']\n",
    "            spatial_feat = graph_data['spatial_feat']\n",
    "            word2vec = graph_data['word2vec']\n",
    "            features, spatial_feat, word2vec, edge_labels = features.to(device), spatial_feat.to(device), word2vec.to(device), edge_labels.to(device)    \n",
    "            \n",
    "            # caption\n",
    "            caption_nodes, caps_gt = cp_data\n",
    "            caption_nodes, caps_gt = caption_nodes.to(device), caps_gt.to(device)\n",
    "            \n",
    "            model.zero_grad()\n",
    "            interaction, caption_output = model( img_loc, det_boxes, caption_nodes, caps_gt, node_num, features, spatial_feat, word2vec, roi_labels)\n",
    "            \n",
    "            # graph loss and acc\n",
    "            interaction = F.softmax(interaction, dim=1)\n",
    "            g_loss = g_criterion(interaction, edge_labels.float())\n",
    "            g_acc = np.sum(np.equal(np.argmax(interaction.cpu().data.numpy(), axis=-1), np.argmax(edge_labels.cpu().data.numpy(), axis=-1)))\n",
    "                    \n",
    "            # caption loss\n",
    "            c_criterion = CELossWithLS(classes=len(text_field.vocab), smoothing=0.1, gamma=0.0, isCos=False, ignore_index=text_field.vocab.stoi['<pad>'])\n",
    "            c_loss = c_criterion(caption_output[:, :-1].contiguous(), caps_gt[:, 1:].contiguous())\n",
    "            \n",
    "            #uda:\n",
    "            #loss = (0.5 * g_loss) + (0.5 * c_loss)\n",
    "            #uda_graph:\n",
    "            loss = g_loss\n",
    "            #uda_caption:\n",
    "            #loss = c_loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            running_g_acc += g_acc\n",
    "            running_edge_count += edge_labels.shape[0]\n",
    "            #break\n",
    "        epoch_loss = running_loss/float(iters)\n",
    "        epoch_g_acc = running_g_acc/float(running_edge_count)\n",
    "        print(\"[{}] Epoch: {}/{} MTL_Loss: {:0.6f} Graph_Acc: {:0.6f}\".format(\\\n",
    "                            'MTL-Train', epoch_count+1, epoch, epoch_loss, epoch_g_acc))\n",
    "        \n",
    "        checkpoint = {'state_dict': model.state_dict()}\n",
    "        save_name = \"checkpoints/mtl_train/UDA_Graph/checkpoint_\" + str(epoch_count+1) + '_epoch.pth'\n",
    "        torch.save(checkpoint, os.path.join(save_name))\n",
    "        \n",
    "        Print(\"=========== Evaluation ===============\")\n",
    "        eval_mtl(model, dict_dataloader_val, text_field)\n",
    "  \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arguments, dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training check for DA_ECBS_ResNet18_09_SC_ECBS\n",
      "Namespace(batch_size=2, cp_annotation_folder='datasets/annotations_new/annotations_SD_inc', cp_cbs='True', cp_cbs_filter='LOG', cp_checkpoint='checkpoints/IDA_MICCAI2021_checkpoints/SD_base_LOG/', cp_decay_epoch=2, cp_features_path='datasets/instruments18/', cp_kernel_sizex=3, cp_kernel_sizey=1, cp_std_factor=0.9, exp_name='m2_transformer', fe_cbs_epoch=5, fe_fil1='LOG', fe_fil2='gau', fe_fil3='gau', fe_kernel_size=3, fe_modelpath='feature_extractor/checkpoint/incremental/inc_ResNet18_SC_CBS_0_012345678.pkl', fe_num_classes=11, fe_std=1.0, fe_std_factor=0.9, fe_use_SC=True, fe_use_cbs=True, gsu_cbs=True, gsu_checkpoint='checkpoints/g_checkpoints/da_ecbs_resnet18_09_SC_eCBS/da_ecbs_resnet18_09_SC_eCBS/epoch_train/checkpoint_D1230_epoch.pth', gsu_feat='resnet18_09_SC_CBS', gsu_file_dir='datasets/instruments18/', gsu_img_dir='left_frames', gsu_w2v_loc='datasets/surgicalscene_word2vec.hdf5', m=40, workers=0)\n",
      "no pre-extracted image featured\n",
      "no pre-extracted image featured\n",
      "no pre-extracted image featured\n",
      "train: 1560\n",
      "val: 447\n",
      "vocabulary size is: 41\n",
      "defaultdict(<function _default_unk_index at 0x7fb9574bd8c8>, {'<unk>': 0, '<pad>': 1, '<bos>': 2, '<eos>': 3, 'is': 4, 'tissue': 5, 'forceps': 6, 'monopolar': 7, 'curved': 8, 'scissors': 9, 'bipolar': 10, 'manipulating': 11, 'and': 12, 'are': 13, 'prograsp': 14, 'cutting': 15, 'idle': 16, 'retracting': 17, 'suction': 18, 'blood': 19, 'suctioning': 20, 'cautery': 21, 'spatulated': 22, 'grasping': 23, 'dissector': 24, 'maryland': 25, 'cauterizing': 26, 'probe': 27, 'ultrasound': 28, 'sensing': 29, 'vessels': 30, 'looping': 31, 'clipping': 32, 'stapler': 33, 'driver': 34, 'large': 35, 'needle': 36, 'applier': 37, 'clip': 38, 'stapling': 39, 'suturing': 40})\n"
     ]
    }
   ],
   "source": [
    "# arguments\n",
    "device = torch.device('cuda')\n",
    "parser = argparse.ArgumentParser(description='Incremental domain adaptation for surgical report generation')\n",
    "parser.add_argument('--batch_size', type=int, default=2)\n",
    "parser.add_argument('--workers', type=int, default=0)\n",
    "\n",
    "# caption\n",
    "parser.add_argument('--exp_name', type=str, default='m2_transformer')\n",
    "parser.add_argument('--m', type=int, default=40)   \n",
    "parser.add_argument('--cp_cbs', type=str, default='True')\n",
    "parser.add_argument('--cp_cbs_filter', default='LOG', type=str) # Potential choice: 'gau' and 'LOG'\n",
    "parser.add_argument('--cp_kernel_sizex', default=3, type=int)\n",
    "parser.add_argument('--cp_kernel_sizey', default=1, type=int)\n",
    "parser.add_argument('--cp_decay_epoch', default=2, type=int) \n",
    "parser.add_argument('--cp_std_factor', default=0.9, type=float)\n",
    "\n",
    "# graph\n",
    "parser.add_argument('--gsu_cbs',        type=bool, default=True)\n",
    "parser.add_argument('--gsu_feat', type=str,  default='resnet18_09_SC_CBS')\n",
    "parser.add_argument('--gsu_w2v_loc', type=str,  default='datasets/surgicalscene_word2vec.hdf5')\n",
    "\n",
    "# feature_extractor\n",
    "parser.add_argument('--fe_use_cbs',            type=bool,      default=True,        help='use CBS')\n",
    "parser.add_argument('--fe_std',                type=float,     default=1.0,         help='')\n",
    "parser.add_argument('--fe_std_factor',         type=float,     default=0.9,         help='')\n",
    "parser.add_argument('--fe_cbs_epoch',          type=int,       default=5,           help='')\n",
    "parser.add_argument('--fe_kernel_size',        type=int,       default=3,           help='')\n",
    "parser.add_argument('--fe_fil1',               type=str,       default='LOG',       help='gau, LOG')\n",
    "parser.add_argument('--fe_fil2',               type=str,       default='gau',       help='gau, LOG')\n",
    "parser.add_argument('--fe_fil3',               type=str,       default='gau',       help='gau, LOG')\n",
    "parser.add_argument('--fe_num_classes',        type=int,       default=11,           help='11')\n",
    "parser.add_argument('--fe_use_SC',             type=bool,      default=True,       help='use SuperCon')\n",
    "\n",
    "# file dirs\n",
    "print('Training check for DA_ECBS_ResNet18_09_SC_ECBS')\n",
    "\n",
    "parser.add_argument('--gsu_img_dir', type=str,  default='left_frames')\n",
    "parser.add_argument('--gsu_file_dir', type=str,  default='datasets/instruments18/')\n",
    "\n",
    "parser.add_argument('--cp_features_path', type=str, default='datasets/instruments18/') \n",
    "parser.add_argument('--cp_annotation_folder', type=str, default='datasets/annotations_new/annotations_SD_inc')\n",
    "\n",
    "# checkpoints dir\n",
    "parser.add_argument('--fe_modelpath',          type=str,       default='feature_extractor/checkpoint/incremental/inc_ResNet18_SC_CBS_0_012345678.pkl')\n",
    "parser.add_argument('--gsu_checkpoint', type=str,  default='checkpoints/g_checkpoints/da_ecbs_resnet18_09_SC_eCBS/da_ecbs_resnet18_09_SC_eCBS/epoch_train/checkpoint_D1230_epoch.pth')\n",
    "parser.add_argument('--cp_checkpoint', type=str, default='checkpoints/IDA_MICCAI2021_checkpoints/SD_base_LOG/')\n",
    "\n",
    "\n",
    "args = parser.parse_args(args=[])\n",
    "print(args)\n",
    "\n",
    "# graph scene understanding constants\n",
    "gsu_const = {}\n",
    "gsu_const['file_dir'] = args.gsu_file_dir\n",
    "gsu_const['img_dir'] = args.gsu_img_dir\n",
    "gsu_const['dataconst'] = SurgicalSceneConstants()\n",
    "gsu_const['feature_extractor'] = args.gsu_feat\n",
    "gsu_const['w2v_loc'] =args.gsu_w2v_loc\n",
    "\n",
    "\n",
    "# Pipeline for image regions and text\n",
    "#image_field = ImageDetectionsField(detections_path=args.cp_features_path, max_detections=6, load_in_tmp=False)  \n",
    "image_field = None\n",
    "text_field = TextField(init_token='<bos>', eos_token='<eos>', lower=True, tokenize='spacy', remove_punctuation=True, nopoints=False)\n",
    "\n",
    "# Create the dataset \n",
    "dataset = COCO(image_field, text_field, gsu_const, args.cp_features_path, args.cp_annotation_folder, args.cp_annotation_folder)\n",
    "train_dataset, val_dataset = dataset.splits   \n",
    "print('train:', len(train_dataset))\n",
    "print('val:', len(val_dataset))\n",
    "    \n",
    "# caption data\n",
    "if not os.path.isfile('datasets/vocab_%s.pkl' % args.exp_name):\n",
    "    print(\"Building vocabulary\")\n",
    "    text_field.build_vocab(train_dataset, val_dataset, min_freq=2)  \n",
    "    pickle.dump(text_field.vocab, open('datasets/vocab_%s.pkl' % args.exp_name, 'wb'))\n",
    "else:\n",
    "    text_field.vocab = pickle.load(open('datasets/vocab_%s.pkl' % args.exp_name, 'rb'))\n",
    "\n",
    "print('vocabulary size is:', len(text_field.vocab))\n",
    "print(text_field.vocab.stoi)\n",
    "\n",
    "# dataset\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=False, num_workers=args.workers)\n",
    "dict_dataset_val = val_dataset.image_dictionary({'image': image_field, 'text': RawField()})\n",
    "dict_dataloader_val = DataLoader(dict_dataset_val, batch_size=args.batch_size) # for caption with word GT class number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net model\n",
    "if args.fe_use_SC: feature_network = SupConResNet(args=args)\n",
    "else: feature_network = ResNet18(args)\n",
    "\n",
    "# CBS\n",
    "if args.fe_use_cbs:\n",
    "    if args.fe_use_SC: feature_network.encoder.get_new_kernels(0)\n",
    "    else: feature_network.get_new_kernels(0)\n",
    "\n",
    "# gpu\n",
    "num_gpu = torch.cuda.device_count()\n",
    "if num_gpu > 0:\n",
    "    device_ids = np.arange(num_gpu).tolist()    \n",
    "    if args.fe_use_SC:\n",
    "        feature_network.encoder = torch.nn.DataParallel(feature_network.encoder)\n",
    "        feature_network = feature_network.cuda()\n",
    "    else:\n",
    "        feature_network = nn.DataParallel(feature_network, device_ids=device_ids).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caption model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MemoryAugmentedEncoder_CBS\n"
     ]
    }
   ],
   "source": [
    "if args.cp_cbs == 'True':\n",
    "    from models.transformer import MemoryAugmentedEncoder_CBS\n",
    "    print(\"MemoryAugmentedEncoder_CBS\")\n",
    "    encoder = MemoryAugmentedEncoder_CBS(3, 0, attention_module=ScaledDotProductAttentionMemory, attention_module_kwargs={'m': args.m})\n",
    "else:\n",
    "    print(\"MemoryAugmentedEncoder\")\n",
    "    encoder = MemoryAugmentedEncoder(3, 0, attention_module=ScaledDotProductAttentionMemory, attention_module_kwargs={'m': args.m}) \n",
    "\n",
    "decoder = MeshedDecoder(len(text_field.vocab), 54, 3, text_field.vocab.stoi['<pad>'])\n",
    "caption_model = Transformer(text_field.vocab.stoi['<bos>'], encoder, decoder).to(device)\n",
    "\n",
    "if args.cp_cbs == 'True':\n",
    "    caption_model.encoder.get_new_kernels(0, args.cp_kernel_sizex, args.cp_kernel_sizey, args.cp_decay_epoch, args.cp_std_factor, args.cp_cbs_filter) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_su_model = AGRNN(bias= True, bn=False, dropout=0.3, multi_attn=False, layer=1, diff_edge=False, use_cbs = args.gsu_cbs)\n",
    "if args.gsu_cbs:\n",
    "    graph_su_model.grnn1.gnn.apply_h_h_edge.get_new_kernels(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Pre-trained_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# caption\n",
    "pretrained_model = torch.load(args.cp_checkpoint+('%s_best.pth' % args.exp_name))\n",
    "caption_model.load_state_dict(pretrained_model['state_dict']) \n",
    "\n",
    "# graph\n",
    "pretrained_model = torch.load(args.gsu_checkpoint)\n",
    "graph_su_model.load_state_dict(pretrained_model['state_dict'])\n",
    "\n",
    "# feature network\n",
    "feature_network.load_state_dict(torch.load(args.fe_modelpath))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# feature extraction layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the encoder layer\n",
    "if args.fe_use_SC:\n",
    "    feature_network = feature_network.encoder\n",
    "else:\n",
    "    if args.fe_use_cbs: feature_network = nn.Sequential(*list(feature_network.module.children())[:-2])\n",
    "    else: feature_network = nn.Sequential(*list(feature_network.module.children())[:-1])\n",
    "\n",
    "feature_network = feature_network.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combined_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mtl_model(feature_network, graph_su_model, caption_model)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['datasets/instruments18/seq_1/left_frames/frame047.png', 'datasets/instruments18/seq_1/left_frames/frame065.png']\n",
      "node_num [4, 3]\n",
      "node_feat torch.Size([7, 512])\n",
      "spatial_deat torch.Size([18, 16])\n",
      "word_2_vec torch.Size([7, 300])\n",
      "roi_labels [array([0, 1, 2, 7]), array([0, 2, 4])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "1it [00:03,  3.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "interaction tensor([[  0.0000,  -8.2871,  -7.3069,  -5.1491,  -4.4096, -23.5156,   0.0000,\n",
      "           0.0000,   0.0000, -32.4802, -24.8784, -35.0060, -13.8209],\n",
      "        [ -1.9168,   0.0000,  -5.8999,   0.0000,  -4.8041, -13.6514,   0.0000,\n",
      "          -4.8086,  -8.7836, -22.3640, -17.1114, -25.4892, -11.4277],\n",
      "        [ -0.2609, -10.1361,  -7.8001,  -2.7731,  -3.8458,  -4.8875,   0.0000,\n",
      "          -7.6525,  -5.9701,   0.0000,  -4.0276, -11.3565,   0.0000],\n",
      "        [ -0.4975,  -4.6083,  -4.3027,  -0.5445, -10.7980,   0.0000, -26.4635,\n",
      "         -11.1808,  -6.9873, -27.4969, -20.9212,   0.0000, -15.3676],\n",
      "        [ -0.5307,  -5.8077,  -6.8055,  -3.4842,  -4.5793,   1.2136,  -6.6921,\n",
      "          -9.9246,  -6.0392,   0.0000,   0.0000,  -7.1559,  -6.2763]],\n",
      "       device='cuda:0')\n",
      "['datasets/instruments18/seq_1/left_frames/frame094.png', 'datasets/instruments18/seq_1/left_frames/frame013.png']\n",
      "node_num [4, 4]\n",
      "node_feat torch.Size([8, 512])\n",
      "spatial_deat torch.Size([24, 16])\n",
      "word_2_vec torch.Size([8, 300])\n",
      "roi_labels [array([0, 1, 2, 4]), array([0, 1, 2, 4])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "2it [00:04,  2.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "interaction tensor([[  1.7616, -10.8580,  -4.5352,  -0.7789,   0.0000, -13.2561, -21.3044,\n",
      "         -23.0021, -12.5584, -23.3428, -25.0887,   0.0000, -19.4127],\n",
      "        [  0.5599,  -7.7693,  -9.9580,   1.1444,   0.0000, -11.7384, -31.9997,\n",
      "          -9.1883,  -5.9409, -29.3164, -15.2528,   0.0000, -16.7993],\n",
      "        [  0.8450,  -9.3496,  -8.1437,  -5.5296,  -8.2990,   0.0000,   0.0000,\n",
      "         -24.0040, -11.2224, -10.5505, -15.7250, -16.0700, -12.9930],\n",
      "        [  0.6087,  -6.8864,  -4.3954,  -1.4028,  -8.0746,   0.0000, -24.9153,\n",
      "         -17.3685,  -5.3791, -25.6285, -22.8299,   0.0000,   0.0000],\n",
      "        [  0.0000,  -4.7851,  -4.7146,   0.4951,  -8.0289, -11.4949,   0.0000,\n",
      "          -8.4013,  -7.4362, -19.6092, -16.5382, -17.3998, -10.8829],\n",
      "        [  1.5607,  -9.5905,  -9.4825,  -3.1004,  -6.5390,   0.0000,   0.0000,\n",
      "         -16.8339,  -9.1139, -11.1966,   0.0000,   0.0000, -11.1149]],\n",
      "       device='cuda:0')\n",
      "['datasets/instruments18/seq_1/left_frames/frame109.png', 'datasets/instruments18/seq_1/left_frames/frame127.png']\n",
      "node_num [4, 4]\n",
      "node_feat torch.Size([8, 512])\n",
      "spatial_deat torch.Size([24, 16])\n",
      "word_2_vec torch.Size([8, 300])\n",
      "roi_labels [array([0, 1, 2, 4]), array([0, 1, 2, 4])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "3it [00:04,  2.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "interaction tensor([[  0.0000,   0.0000,   0.0000,  -5.6596,  -9.1584, -22.0826,   0.0000,\n",
      "         -15.3100,   0.0000, -33.0210, -32.8981, -34.6749, -21.1730],\n",
      "        [  2.2521, -14.1995,   0.0000,  -8.0715,  -8.3112, -37.4496,   0.0000,\n",
      "          -9.3564, -15.5919, -59.0969, -43.6544, -61.7319,   0.0000],\n",
      "        [  1.9643, -10.2696,  -8.6664,   0.0000,  -5.8914,   0.0000,   0.0000,\n",
      "           0.0000, -10.7342, -10.2574, -13.6837, -15.1018, -10.5569],\n",
      "        [  0.7186,  -4.3310,   0.0000,  -2.6788,  -5.0853,   0.0000, -20.0343,\n",
      "          -9.0862,  -5.6965, -23.7891,   0.0000, -21.1476, -11.9725],\n",
      "        [ -0.2177,  -5.3122,  -5.7098,   0.0000,  -7.3874, -15.4011, -27.3393,\n",
      "           0.0000,  -7.6885, -26.7382, -18.7629, -27.4717, -14.5518],\n",
      "        [ -0.3362,  -9.5501,   0.0000,  -4.4658,  -6.1753,   1.0032,  -6.2940,\n",
      "           0.0000, -10.5625,  -5.6543, -10.1116, -10.1616,  -7.4469]],\n",
      "       device='cuda:0')\n",
      "['datasets/instruments18/seq_1/left_frames/frame135.png', 'datasets/instruments18/seq_1/left_frames/frame070.png']\n",
      "node_num [5, 3]\n",
      "node_feat torch.Size([8, 512])\n",
      "spatial_deat torch.Size([26, 16])\n",
      "word_2_vec torch.Size([8, 300])\n",
      "roi_labels [array([0, 1, 2, 4, 6]), array([0, 2, 4])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "4it [00:05,  1.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "interaction tensor([[ 3.1906e-02, -9.4376e+00,  0.0000e+00, -1.0439e+00, -1.1433e+01,\n",
      "         -2.3973e+01, -3.5077e+01, -1.5541e+01, -1.2406e+01, -3.4316e+01,\n",
      "         -3.0824e+01,  0.0000e+00, -1.9082e+01],\n",
      "        [-2.5102e+00, -6.0729e+00, -6.0301e+00,  4.1442e+00,  0.0000e+00,\n",
      "         -1.6117e+01, -2.7929e+01,  0.0000e+00, -6.5825e+00,  0.0000e+00,\n",
      "         -1.6959e+01, -2.4546e+01, -1.7679e+01],\n",
      "        [-3.6221e-02, -6.7651e+00, -7.2599e+00, -3.8176e+00,  0.0000e+00,\n",
      "         -3.6654e+00,  0.0000e+00, -3.4733e+00, -5.3455e+00,  0.0000e+00,\n",
      "         -6.1087e+00, -1.0234e+01, -7.5311e+00],\n",
      "        [ 0.0000e+00, -1.5780e+01, -1.3545e+01, -3.7561e+00, -6.7560e+00,\n",
      "         -8.2817e+00,  0.0000e+00,  0.0000e+00, -9.8606e+00,  0.0000e+00,\n",
      "          0.0000e+00, -1.6347e+01,  0.0000e+00],\n",
      "        [-4.6151e-01, -4.9866e+00, -5.1990e+00,  9.4513e-01, -8.7835e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00, -7.6741e+00, -2.7490e+01,\n",
      "         -1.8997e+01, -2.4490e+01, -1.4884e+01],\n",
      "        [-1.0083e+00, -5.6549e+00, -4.5273e+00, -3.1898e+00, -4.8682e+00,\n",
      "          9.0488e-01, -5.5716e+00, -8.8842e+00, -6.4281e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00, -7.1270e+00]], device='cuda:0')\n",
      "['datasets/instruments18/seq_1/left_frames/frame040.png', 'datasets/instruments18/seq_1/left_frames/frame066.png']\n",
      "node_num [4, 3]\n",
      "node_feat torch.Size([7, 512])\n",
      "spatial_deat torch.Size([18, 16])\n",
      "word_2_vec torch.Size([7, 300])\n",
      "roi_labels [array([0, 1, 2, 4]), array([0, 2, 4])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "5it [00:05,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "interaction tensor([[  0.1032,   0.0000,  -2.1698,  -2.4776,  -4.9631, -10.4636, -19.8205,\n",
      "           0.0000,   0.0000, -19.9683,   0.0000, -19.4585,   0.0000],\n",
      "        [ -0.7767,  -2.8128,  -3.5627,   0.0860,   0.0000,  -5.0281, -11.9112,\n",
      "          -6.2590,   0.0000, -12.1248,  -8.9779,   0.0000,  -6.9479],\n",
      "        [  0.8986, -10.4317,  -8.8533,  -6.6280,   0.0000,   0.0000, -15.0635,\n",
      "           0.0000, -15.5472, -12.8866, -20.3637, -21.0653,   0.0000],\n",
      "        [  0.0000,  -5.6923,   0.0000,  -1.2671,  -5.9448,   0.0000,   0.0000,\n",
      "         -10.3988,  -8.6262,   0.0000, -19.9523, -24.6381, -15.4203],\n",
      "        [  0.0000,  -7.8105,   0.0000,  -4.1904,  -8.4225,   0.0000,   0.0000,\n",
      "          -9.1533,   0.0000,  -6.7394,   0.0000,   0.0000,  -7.3752]],\n",
      "       device='cuda:0')\n",
      "['datasets/instruments18/seq_1/left_frames/frame130.png', 'datasets/instruments18/seq_1/left_frames/frame083.png']\n",
      "node_num [4, 4]\n",
      "node_feat torch.Size([8, 512])\n",
      "spatial_deat torch.Size([24, 16])\n",
      "word_2_vec torch.Size([8, 300])\n",
      "roi_labels [array([0, 1, 2, 4]), array([0, 2, 4, 6])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:05,  1.19s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-464218006533>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# initial network evaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0meval_mtl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict_dataloader_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_field\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# train for 100 epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# train(100, 0.001, model, train_dataloader, dict_dataloader_val, text_field)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-e28a0a6a247b>\u001b[0m in \u001b[0;36meval_mtl\u001b[0;34m(model, dataloader, text_field)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0mg_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaption_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_loc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdet_boxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaps_gt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspatial_feat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword2vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroi_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_field\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_field\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mg_logits_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-069deb6e6702>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, img_dir, det_boxes_all, caps_gt, node_num, features, spatial_feat, word2vec, roi_labels, val, text_field)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mval\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m             \u001b[0mcaption_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaption\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeam_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcp_node_feat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_field\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstoi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'<eos>'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0mcaption_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcp_node_feat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaps_gt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/mobarak/data/lalith/mtl_graph_and_caption/models/captioning_model.py\u001b[0m in \u001b[0;36mbeam_search\u001b[0;34m(self, visual, max_len, eos_idx, beam_size, out_size, return_probs, **kwargs)\u001b[0m\n\u001b[1;32m     69\u001b[0m                     return_probs=False, **kwargs): \n\u001b[1;32m     70\u001b[0m         \u001b[0mbs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeamSearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meos_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeam_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvisual\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/media/mobarak/data/lalith/mtl_graph_and_caption/models/beam_search/beam_search.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, visual, out_size, return_probs, **kwargs)\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatefulness\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb_s\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m                 \u001b[0mvisual\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisual\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;31m# Sort result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/mobarak/data/lalith/mtl_graph_and_caption/models/beam_search/beam_search.py\u001b[0m in \u001b[0;36miter\u001b[0;34m(self, t, visual, outputs, return_probs, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_to_states\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_expand_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mselected_beam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_beam_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0mvisual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_expand_visual\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvisual\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_beam_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mselected_beam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/mobarak/data/lalith/mtl_graph_and_caption/models/containers.py\u001b[0m in \u001b[0;36mapply_to_states\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m                 \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_to_states\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# initial network evaluation\n",
    "eval_mtl(model, dict_dataloader_val, text_field)\n",
    "\n",
    "# train for 100 epoch\n",
    "# train(100, 0.001, model, train_dataloader, dict_dataloader_val, text_field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
