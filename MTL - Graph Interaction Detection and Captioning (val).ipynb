{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pycocotools'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-f95136e2dd6c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mevaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexample\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mExample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnostdout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfield\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImageDetectionsField\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTextField\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRawField\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/mobarak/data/lalith/mtl_graph_and_caption/data/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mfield\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRawField\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMerge\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mImageDetectionsField\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTextField\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCOCO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataLoader\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mTorchDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTorchDataLoader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/mobarak/data/lalith/mtl_graph_and_caption/data/field.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshutil\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVocab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/mobarak/data/lalith/mtl_graph_and_caption/data/dataset.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mexample\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mExample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnostdout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpycocotools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoco\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCOCO\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpyCOCO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pycocotools'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "\n",
    "import json\n",
    "import h5py\n",
    "import itertools\n",
    "import numpy as np\n",
    "import argparse, pickle\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset as torchDataset\n",
    "from torch.utils.data import DataLoader as TorchDataLoader\n",
    "\n",
    "# caption libraries\n",
    "import evaluation\n",
    "import collections\n",
    "from data.example import Example\n",
    "from data.utils import nostdout\n",
    "from data.field import ImageDetectionsField, TextField, RawField\n",
    "from models.transformer import Transformer, MemoryAugmentedEncoder, MeshedDecoder, ScaledDotProductAttentionMemory\n",
    "\n",
    "# graph libraries\n",
    "import utils.io as io\n",
    "import matplotlib.pyplot as plt\n",
    "from utils.g_vis_img import *\n",
    "from models.graph_su import *\n",
    "from evaluation.graph_eval import *\n",
    "\n",
    "\n",
    "# Random seeds\n",
    "seed = 1234\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SurgicalSceneConstants():\n",
    "    '''\n",
    "    Surgical Scene constants\n",
    "    '''\n",
    "    def __init__( self):\n",
    "        self.instrument_classes = ('kidney', 'bipolar_forceps', 'prograsp_forceps', 'large_needle_driver',\n",
    "                                'monopolar_curved_scissors', 'ultrasound_probe', 'suction', 'clip_applier',\n",
    "                                'stapler', 'maryland_dissector', 'spatulated_monopolar_cautery')\n",
    "        self.action_classes = ( 'Idle', 'Grasping', 'Retraction', 'Tissue_Manipulation', \n",
    "                                'Tool_Manipulation', 'Cutting', 'Cauterization', \n",
    "                                'Suction', 'Looping', 'Suturing', 'Clipping', 'Staple', \n",
    "                                'Ultrasound_Sensing')        \n",
    "        #self.file_dir = 'datasets/instruments18/'\n",
    "        #self.word2vec_loc = 'datasets/surgicalscene_word2vec.hdf5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-entropy loss with label smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CELossWithLS(torch.nn.Module):\n",
    "    '''\n",
    "    label smoothing cross-entropy loss for captioning\n",
    "    '''\n",
    "    def __init__(self, classes=None, smoothing=0.1, gamma=3.0, isCos=True, ignore_index=-1):\n",
    "        super(CELossWithLS, self).__init__()\n",
    "        self.complement = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.cls = classes\n",
    "        self.log_softmax = torch.nn.LogSoftmax(dim=1)\n",
    "        self.gamma = gamma\n",
    "        self.ignore_index = ignore_index\n",
    "\n",
    "    def forward(self, logits, target):\n",
    "        with torch.no_grad():\n",
    "            oh_labels = F.one_hot(target.to(torch.int64), num_classes = self.cls).permute(0,1,2).contiguous()\n",
    "            smoothen_ohlabel = oh_labels * self.complement + self.smoothing / self.cls\n",
    "\n",
    "        logs = self.log_softmax(logits[target!=self.ignore_index])\n",
    "        pt = torch.exp(logs)\n",
    "        return -torch.sum((1-pt).pow(self.gamma)*logs * smoothen_ohlabel[target!=self.ignore_index], dim=1).mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' ---------------------------------------- caption dataloader objects ----------------------------------------'''\n",
    "class DataLoader(TorchDataLoader):\n",
    "    def __init__(self, dataset, *args, **kwargs):\n",
    "        super(DataLoader, self).__init__(dataset, *args, collate_fn=dataset.collate_fn(), **kwargs)\n",
    "\n",
    "class Dataset(object):\n",
    "    def __init__(self, examples, fields):\n",
    "        self.examples = examples\n",
    "        self.fields = dict(fields)  \n",
    "\n",
    "    def collate_fn(self):\n",
    "        def collate(batch):\n",
    "            if len(self.fields) == 1:\n",
    "                batch = [batch, ]\n",
    "            else:\n",
    "                batch = list(zip(*batch))\n",
    "\n",
    "            tensors = []\n",
    "            for field, data in zip(self.fields.values(), batch):\n",
    "                tensor = field.process(data)\n",
    "                if isinstance(tensor, collections.Sequence) and any(isinstance(t, torch.Tensor) for t in tensor):\n",
    "                    tensors.extend(tensor)\n",
    "                else:\n",
    "                    tensors.append(tensor)\n",
    "\n",
    "            if len(tensors) > 1:\n",
    "                return tensors\n",
    "            else:\n",
    "                return tensors[0]\n",
    "\n",
    "        return collate\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        example = self.examples[i]\n",
    "        data = []\n",
    "        for field_name, field in self.fields.items():\n",
    "            #if field_name == 'image': print(getattr(example, field_name))\n",
    "            data.append(field.preprocess(getattr(example, field_name)))   \n",
    "\n",
    "        if len(data) == 1:\n",
    "            data = data[0]\n",
    "        return data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getattr__(self, attr):\n",
    "        if attr in self.fields:\n",
    "            for x in self.examples:\n",
    "                yield getattr(x, attr)\n",
    "\n",
    "class ValueDataset(Dataset):\n",
    "    def __init__(self, examples, fields, dictionary):\n",
    "        self.dictionary = dictionary\n",
    "        super(ValueDataset, self).__init__(examples, fields)\n",
    "\n",
    "    def collate_fn(self):\n",
    "        def collate(batch):\n",
    "            value_batch_flattened = list(itertools.chain(*batch))\n",
    "            value_tensors_flattened = super(ValueDataset, self).collate_fn()(value_batch_flattened)\n",
    "\n",
    "            lengths = [0, ] + list(itertools.accumulate([len(x) for x in batch]))\n",
    "            if isinstance(value_tensors_flattened, collections.Sequence) \\\n",
    "                    and any(isinstance(t, torch.Tensor) for t in value_tensors_flattened):\n",
    "                value_tensors = [[vt[s:e] for (s, e) in zip(lengths[:-1], lengths[1:])] for vt in value_tensors_flattened]\n",
    "            else:\n",
    "                value_tensors = [value_tensors_flattened[s:e] for (s, e) in zip(lengths[:-1], lengths[1:])]\n",
    "\n",
    "            return value_tensors\n",
    "        return collate\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        if i not in self.dictionary:\n",
    "            raise IndexError\n",
    "\n",
    "        values_data = []\n",
    "        for idx in self.dictionary[i]:\n",
    "            value_data = super(ValueDataset, self).__getitem__(idx)\n",
    "            values_data.append(value_data)\n",
    "        return values_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dictionary)\n",
    "\n",
    "''' ---------------------------------------- caption dataloader objects ----------------------------------------'''\n",
    "''' ---------------------------------------- GraphSU dataloader objects ----------------------------------------'''    \n",
    "\n",
    "class GraphSUDataset(torchDataset):\n",
    "    \n",
    "    ''' Data loader for graph Scene Understanding'''\n",
    "    def __init__(self,examples, fields, file_dir, img_dir, w2v_loc, dataconst, feature_extractor):\n",
    "        \n",
    "        self.examples = examples\n",
    "        self.fields = dict(fields)\n",
    "\n",
    "        self.file_dir = file_dir\n",
    "        self.img_dir = img_dir\n",
    "        self.dataconst = dataconst\n",
    "        self.feature_extractor = feature_extractor\n",
    "        print('filename', w2v_loc)\n",
    "        self.word2vec = h5py.File(w2v_loc, 'r')\n",
    "    \n",
    "    # word2vec\n",
    "    def _get_word2vec(self,node_ids):\n",
    "        word2vec = np.empty((0,300))\n",
    "        for node_id in node_ids:\n",
    "            vec = self.word2vec[self.dataconst.instrument_classes[node_id]]\n",
    "            word2vec = np.vstack((word2vec, vec))\n",
    "        return word2vec\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getattr__(self, attr):\n",
    "        if attr in self.fields:\n",
    "            for x in self.examples:\n",
    "                yield getattr(x, attr)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        example = self.examples[idx]\n",
    "        frame_path = getattr(example, 'image')\n",
    "        frame_path = frame_path.split(\"/\")\n",
    "        \n",
    "        _img_loc = os.path.join(self.file_dir, frame_path[0],self.img_dir,frame_path[3].split(\"_\")[0]+'.png')\n",
    "        frame_data = h5py.File(os.path.join(self.file_dir, frame_path[0],'vsgat',self.feature_extractor, frame_path[3].split(\"_\")[0]+'_features.hdf5'), 'r')    \n",
    "        \n",
    "        data = {}\n",
    "        data['img_name'] = frame_data['img_name'].value[:] + '.jpg'\n",
    "        data['img_loc'] = _img_loc\n",
    "        \n",
    "        data['node_num'] = frame_data['node_num'].value\n",
    "        data['roi_labels'] = frame_data['classes'][:]\n",
    "        data['det_boxes'] = frame_data['boxes'][:]\n",
    "        \n",
    "        data['edge_labels'] = frame_data['edge_labels'][:]\n",
    "        data['edge_num'] = data['edge_labels'].shape[0]\n",
    "        \n",
    "        data['features'] = frame_data['node_features'][:]\n",
    "        data['spatial_feat'] = frame_data['spatial_features'][:]\n",
    "        \n",
    "        \n",
    "        data['word2vec'] = self._get_word2vec(data['roi_labels'])\n",
    "        return data\n",
    "\n",
    "    def collate_fn(self):\n",
    "        def collate(batch):\n",
    "            batch_data = {}\n",
    "            batch_data['img_name'] = []\n",
    "            batch_data['img_loc'] = []\n",
    "            batch_data['node_num'] = []\n",
    "            batch_data['roi_labels'] = []\n",
    "            batch_data['det_boxes'] = []\n",
    "            batch_data['edge_labels'] = []\n",
    "            batch_data['edge_num'] = []\n",
    "            batch_data['features'] = []\n",
    "            batch_data['spatial_feat'] = []\n",
    "            batch_data['word2vec'] = []\n",
    "\n",
    "            for data in batch:\n",
    "                batch_data['img_name'].append(data['img_name'])\n",
    "                batch_data['img_loc'].append(data['img_loc'])\n",
    "                batch_data['node_num'].append(data['node_num'])\n",
    "                batch_data['roi_labels'].append(data['roi_labels'])\n",
    "                batch_data['det_boxes'].append(data['det_boxes'])\n",
    "                batch_data['edge_labels'].append(data['edge_labels'])\n",
    "                batch_data['edge_num'].append(data['edge_num'])\n",
    "                batch_data['features'].append(data['features'])\n",
    "                batch_data['spatial_feat'].append(data['spatial_feat'])\n",
    "                batch_data['word2vec'].append(data['word2vec'])\n",
    "\n",
    "            batch_data['edge_labels'] = torch.FloatTensor(np.concatenate(batch_data['edge_labels'], axis=0))\n",
    "            batch_data['features'] = torch.FloatTensor(np.concatenate(batch_data['features'], axis=0))\n",
    "            batch_data['spatial_feat'] = torch.FloatTensor(np.concatenate(batch_data['spatial_feat'], axis=0))\n",
    "            batch_data['word2vec'] = torch.FloatTensor(np.concatenate(batch_data['word2vec'], axis=0))\n",
    "            return batch_data\n",
    "\n",
    "        return collate\n",
    "''' ---------------------------------------- GraphSU dataloader objects ----------------------------------------'''\n",
    "class DictionaryDataset(Dataset):\n",
    "    def __init__(self, examples, fields, key_fields, gsu_file_dir='', gsu_img_dir='', gsu_w2v_loc='', gsu_feat = ''):\n",
    "        if not isinstance(key_fields, (tuple, list)):\n",
    "            key_fields = (key_fields,)\n",
    "        for field in key_fields:\n",
    "            assert (field in fields)\n",
    "\n",
    "        dictionary = collections.defaultdict(list)\n",
    "        key_fields = {k: fields[k] for k in key_fields}\n",
    "        value_fields = {k: fields[k] for k in fields.keys() if k not in key_fields}\n",
    "        key_examples = []\n",
    "        key_dict = dict()\n",
    "        value_examples = []\n",
    "\n",
    "        for i, e in enumerate(examples):\n",
    "            key_example = Example.fromdict({k: getattr(e, k) for k in key_fields})\n",
    "            value_example = Example.fromdict({v: getattr(e, v) for v in value_fields})\n",
    "            if key_example not in key_dict:\n",
    "                key_dict[key_example] = len(key_examples)\n",
    "                key_examples.append(key_example)\n",
    "\n",
    "            value_examples.append(value_example)\n",
    "            dictionary[key_dict[key_example]].append(i)\n",
    "\n",
    "        self.key_dataset = Dataset(key_examples, key_fields)\n",
    "        self.value_dataset = ValueDataset(value_examples, value_fields, dictionary)\n",
    "\n",
    "        dataconst = SurgicalSceneConstants()\n",
    "        #self.graph_su_dataset = GraphSUDataset(key_examples, key_fields, 'left_frames', dataconst, 'resnet18_11_cbs_ts')\n",
    "        self.graph_su_dataset = GraphSUDataset(key_examples, key_fields, gsu_file_dir, gsu_img_dir, gsu_w2v_loc, dataconst, gsu_feat)\n",
    "                                                                        \n",
    "        \n",
    "        super(DictionaryDataset, self).__init__(examples, fields)\n",
    "\n",
    "    def collate_fn(self):\n",
    "        def collate(batch):\n",
    "            key_batch, value_batch, graph_su_batch = list(zip(*batch))\n",
    "            key_tensors = self.key_dataset.collate_fn()(key_batch)\n",
    "            value_tensors = self.value_dataset.collate_fn()(value_batch)\n",
    "            graph_su_tensors = self.graph_su_dataset.collate_fn()(graph_su_batch)\n",
    "            return key_tensors, value_tensors, graph_su_tensors\n",
    "        return collate\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.key_dataset[i], self.value_dataset[i], self.graph_su_dataset[i]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.key_dataset)\n",
    "\n",
    "class PairedDataset(Dataset):\n",
    "    def __init__(self, examples, fields, gsu_file_dir='', gsu_img_dir='', gsu_w2v_loc='', gsu_feat=''):\n",
    "        assert ('image' in fields)\n",
    "        assert ('text' in fields)\n",
    "        super(PairedDataset, self).__init__(examples, fields)\n",
    "        self.image_field = self.fields['image']\n",
    "        self.text_field = self.fields['text']\n",
    "        self.gsu_file_dir = gsu_file_dir\n",
    "        self.gsu_img_dir = gsu_img_dir\n",
    "        self.gsu_w2v_loc = gsu_w2v_loc\n",
    "        self.gsu_feat = gsu_feat\n",
    "        \n",
    "    def image_dictionary(self, fields=None):\n",
    "        if not fields:\n",
    "            fields = self.fields\n",
    "        dataset = DictionaryDataset(self.examples, fields, 'image', self.gsu_file_dir, self.gsu_img_dir, self.gsu_w2v_loc, self.gsu_feat)\n",
    "        return dataset\n",
    "\n",
    "\n",
    "class COCO(PairedDataset):\n",
    "    def __init__(self, image_field, text_field, img_root, ann_root, id_root=None, gsu_file_dir='', gsu_img_dir='', gsu_w2v_loc='', gsu_feat=''):\n",
    "        roots = {}\n",
    "        roots['train'] = {\n",
    "            'img': img_root,\n",
    "            'cap': os.path.join(ann_root, 'captions_train.json')  \n",
    "        }\n",
    "        roots['val'] = {\n",
    "            'img': img_root,\n",
    "            'cap': os.path.join(ann_root, 'captions_val.json')\n",
    "        }\n",
    "        \n",
    "        self.gsu_file_dir = gsu_file_dir\n",
    "        self.gsu_img_dir = gsu_img_dir\n",
    "        self.gsu_feat = gsu_feat\n",
    "        self.gsu_w2v_loc = gsu_w2v_loc\n",
    "        \n",
    "        if id_root is not None:\n",
    "            ids = {}\n",
    "            ids['train'] = json.load(open(os.path.join(id_root, 'WithCaption_id_path_train.json'), 'r'))\n",
    "            ids['val'] = json.load(open(os.path.join(id_root, 'WithCaption_id_path_val.json'), 'r'))   \n",
    "        else:\n",
    "            ids = None\n",
    "        \n",
    "        with nostdout():\n",
    "            self.train_examples, self.val_examples = self.get_samples(roots, ids)\n",
    "        examples = self.train_examples + self.val_examples\n",
    "        super(COCO, self).__init__(examples, {'image': image_field, 'text': text_field},self.gsu_file_dir, self.gsu_img_dir, self.gsu_w2v_loc, self.gsu_feat)   \n",
    "\n",
    "    @property\n",
    "    def splits(self):\n",
    "        train_split = PairedDataset(self.train_examples, self.fields, self.gsu_file_dir, self.gsu_img_dir, self.gsu_w2v_loc, self.gsu_feat) \n",
    "        val_split = PairedDataset(self.val_examples, self.fields, self.gsu_file_dir, self.gsu_img_dir, self.gsu_w2v_loc, self.gsu_feat)\n",
    "        return train_split, val_split\n",
    "\n",
    "    @classmethod\n",
    "    def get_samples(cls, roots, ids_dataset=None):\n",
    "        train_samples = []\n",
    "        val_samples = []\n",
    "   \n",
    "        for split in ['train', 'val']:\n",
    "            anns = json.load(open(roots[split]['cap'], 'r'))\n",
    "            if ids_dataset is not None:\n",
    "                ids = ids_dataset[split]\n",
    "                \n",
    "            for index in range(len(ids)):              \n",
    "                id_path = ids[index]  \n",
    "                caption = anns[index]['caption']\n",
    "                #example = Example.fromdict({'image': os.path.join(roots[split]['img'], id_path), 'text': caption})\n",
    "                example = Example.fromdict({'image': os.path.join('', id_path), 'text': caption})\n",
    "                if split == 'train':\n",
    "                    train_samples.append(example)\n",
    "                elif split == 'val':\n",
    "                    val_samples.append(example)\n",
    "                \n",
    "        return train_samples, val_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MTL Model (Graph Scene Understanding and Captioning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mtl_model(nn.Module):\n",
    "    '''\n",
    "    Multi-task model : Graph Scene Understanding and Captioning\n",
    "    '''\n",
    "    def __init__(self, caption, graph):\n",
    "        super(mtl_model, self).__init__()\n",
    "        self.caption = caption\n",
    "        self.graph_su = graph\n",
    "\n",
    "    def forward(self, detections, captions, batch_graph, batch_h_node_list, batch_obj_node_list, batch_h_h_e_list, batch_o_o_e_list, batch_h_o_e_list, feat, spatial_feat, word2vec):\n",
    "        caption_output = self.caption(detections, captions)\n",
    "        interaction = self.graph_su(batch_graph, batch_h_node_list, batch_obj_node_list, batch_h_h_e_list, batch_o_o_e_list, batch_h_o_e_list, feat, spatial_feat, word2vec)\n",
    "        return caption_output, interaction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Matrix : Graph (Loss, Acc, ECE), Caption (Brier, Cider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_metrics(model, device, dataloader, text_field, g_temp = 1.5, c_temp = None):\n",
    "    import itertools\n",
    "    \n",
    "    model.caption.decoder.caption_ts = c_temp\n",
    "    #model.to(device)\n",
    "    \n",
    "    model.eval()\n",
    "    gen = {}\n",
    "    gts = {}\n",
    "\n",
    "    # graph\n",
    "    # criterion and scheduler\n",
    "    g_criterion = nn.MultiLabelSoftMarginLoss()                   \n",
    "    g_edge_count = 0\n",
    "    g_total_acc = 0.0\n",
    "    g_total_loss = 0.0\n",
    "    g_logits_list = []\n",
    "    g_labels_list = []\n",
    "\n",
    "    #print(model.caption.beam_search)\n",
    "    \n",
    "    with tqdm(desc='evaluation', unit='it', total=len(dataloader)) as pbar:\n",
    "        for it, (images, caps_gt, graph_data) in enumerate(iter(dataloader)):\n",
    "            \n",
    "            # graph\n",
    "            img_name = graph_data['img_name']\n",
    "            img_loc = graph_data['img_loc']\n",
    "            node_num = graph_data['node_num']\n",
    "            roi_labels = graph_data['roi_labels']\n",
    "            det_boxes = graph_data['det_boxes']\n",
    "            edge_labels = graph_data['edge_labels']\n",
    "            edge_num = graph_data['edge_num']\n",
    "            features = graph_data['features']\n",
    "            spatial_feat = graph_data['spatial_feat']\n",
    "            word2vec = graph_data['word2vec']\n",
    "            features, spatial_feat, word2vec, edge_labels = features.to(device), spatial_feat.to(device), word2vec.to(device), edge_labels.to(device)    \n",
    "        \n",
    "            # caption\n",
    "            images = images.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # caption\n",
    "                caption_out, _ = model.caption.beam_search(images, 20, text_field.vocab.stoi['<eos>'], 5, out_size=1)\n",
    "                #print(caption_out)\n",
    "                # graph\n",
    "                g_output = model.graph_su(node_num, features, spatial_feat, word2vec, roi_labels, validation=True)\n",
    "                g_output = g_output/g_temp\n",
    "                g_logits_list.append(g_output)\n",
    "                g_labels_list.append(edge_labels)\n",
    "                # loss and accuracy\n",
    "                g_loss = g_criterion(g_output, edge_labels.float())\n",
    "                g_acc = np.sum(np.equal(np.argmax(g_output.cpu().data.numpy(), axis=-1), np.argmax(edge_labels.cpu().data.numpy(), axis=-1)))\n",
    "            \n",
    "            # accumulate loss and accuracy of the batch\n",
    "            g_total_loss += g_loss.item() * edge_labels.shape[0]\n",
    "            g_total_acc  += g_acc\n",
    "            g_edge_count += edge_labels.shape[0]\n",
    "            \n",
    "            caps_gen = text_field.decode(caption_out, join_words=False)\n",
    "            for i, (gts_i, gen_i) in enumerate(zip(caps_gt, caps_gen)):\n",
    "                gen_i = ' '.join([k for k, g in itertools.groupby(gen_i)])\n",
    "                gen['%d_%d' % (it, i)] = [gen_i, ]    \n",
    "                gts['%d_%d' % (it, i)] = gts_i\n",
    "            pbar.update()\n",
    "    \n",
    "    #graph loss\n",
    "    g_logits_all = torch.cat(g_logits_list).cuda()\n",
    "    g_labels_all = torch.cat(g_labels_list).cuda()\n",
    "    g_total_acc = g_total_acc / g_edge_count\n",
    "    g_total_loss = g_total_loss / len(dataloader)\n",
    "\n",
    "    g_logits_all = F.softmax(g_logits_all, dim=1)\n",
    "    g_map_value, g_ece, g_sce, g_tace, g_brier, g_uce = calibration_metrics(g_logits_all, g_labels_all, 'test')\n",
    "    print('acc: %0.6f map: %0.6f loss: %0.6f, ece:%0.6f, sce:%0.6f, tace:%0.6f, brier:%.6f, uce:%.6f' %(g_total_acc, g_map_value, g_total_loss, g_ece, g_sce, g_tace, g_brier, g_uce.item()) )\n",
    "\n",
    "\n",
    "    if not os.path.exists('results/c_results/predict_caption'):\n",
    "        os.makedirs('results/c_results/predict_caption')\n",
    "    json.dump(gen, open('results/c_results/predict_caption/predict_caption_val.json', 'w'))\n",
    "\n",
    "    gts = evaluation.PTBTokenizer.tokenize(gts)\n",
    "    gen = evaluation.PTBTokenizer.tokenize(gen)\n",
    "    scores, _ = evaluation.compute_scores(gts, gen)\n",
    "    return scores, g_total_acc, g_map_value, g_total_loss, g_ece, g_sce, g_tace, g_brier, g_uce.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.0: MTL Model (CBS, Incremental Learning) Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation\n",
      "Namespace(batch_size=1, cp_annotation_folder='datasets/annotations_new/annotations_SD_inc', cp_cbs='True', cp_cbs_filter='LOG', cp_checkpoint='checkpoints/IDA_MICCAI2021_checkpoints/inc_SC_eCBS/', cp_decay_epoch=2, cp_features_path='datasets/instruments18/', cp_kernel_sizex=3, cp_kernel_sizey=1, cp_std_factor=0.9, exp_name='m2_transformer', gsu_cbs=True, gsu_checkpoint='checkpoints/g_checkpoints/d2g_ecbs_resnet18_11_SC_eCBS/d2g_ecbs_resnet18_11_SC_eCBS/epoch_train/checkpoint_D2F70_epoch.pth', gsu_feat='resnet18_11_SC_CBS', gsu_file_dir='datasets/instruments18/', gsu_img_dir='left_frames', gsu_w2v_loc='datasets/surgicalscene_word2vec.hdf5', m=40, workers=0)\n",
      "train: 1560\n",
      "val: 447\n",
      "vocabulary size is: 41\n",
      "defaultdict(<function _default_unk_index at 0x7f78addd9a60>, {'<unk>': 0, '<pad>': 1, '<bos>': 2, '<eos>': 3, 'is': 4, 'tissue': 5, 'forceps': 6, 'monopolar': 7, 'curved': 8, 'scissors': 9, 'bipolar': 10, 'manipulating': 11, 'and': 12, 'are': 13, 'prograsp': 14, 'cutting': 15, 'idle': 16, 'retracting': 17, 'suction': 18, 'blood': 19, 'suctioning': 20, 'cautery': 21, 'spatulated': 22, 'grasping': 23, 'dissector': 24, 'maryland': 25, 'cauterizing': 26, 'probe': 27, 'ultrasound': 28, 'sensing': 29, 'vessels': 30, 'looping': 31, 'clipping': 32, 'stapler': 33, 'driver': 34, 'large': 35, 'needle': 36, 'applier': 37, 'clip': 38, 'stapling': 39, 'suturing': 40})\n",
      "MemoryAugmentedEncoder_CBS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "evaluation:   0%|          | 0/447 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filename datasets/surgicalscene_word2vec.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluation: 100%|██████████| 447/447 [01:54<00:00,  3.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.571059 map: 0.321686 loss: 0.509024, ece:0.219497, sce:0.047773, tace:0.049699, brier:0.649657, uce:0.283210\n",
      "Initial Graph SU: acc: 0.571059 map: 0.321686 loss: 0.509024, ece:0.219497, sce:0.047773, tace:0.049699, brier:0.649657, uce:0.283210\n",
      "Initial Caption scores : {'BLEU': array([0.5498, 0.4714, 0.4238, 0.3801]), 'METEOR': 0.2861, 'ROUGE': 0.57, 'CIDEr': 2.7487}\n"
     ]
    }
   ],
   "source": [
    "#if __name__ == '__main__':\n",
    "\n",
    "print('Validation')\n",
    "\n",
    "# arguments\n",
    "device = torch.device('cuda')\n",
    "parser = argparse.ArgumentParser(description='Incremental domain adaptation for surgical report generation')\n",
    "parser.add_argument('--batch_size', type=int, default=1)\n",
    "parser.add_argument('--workers', type=int, default=0)\n",
    "\n",
    "# caption\n",
    "parser.add_argument('--exp_name', type=str, default='m2_transformer')\n",
    "parser.add_argument('--m', type=int, default=40)   \n",
    "parser.add_argument('--cp_cbs', type=str, default='True')\n",
    "parser.add_argument('--cp_cbs_filter', default='LOG', type=str) # Potential choice: 'gau' and 'LOG'\n",
    "parser.add_argument('--cp_kernel_sizex', default=3, type=int)\n",
    "parser.add_argument('--cp_kernel_sizey', default=1, type=int)\n",
    "parser.add_argument('--cp_decay_epoch', default=2, type=int) \n",
    "parser.add_argument('--cp_std_factor', default=0.9, type=float)\n",
    "parser.add_argument('--cp_features_path', type=str, default='datasets/instruments18/') \n",
    "parser.add_argument('--cp_annotation_folder', type=str, default='datasets/annotations_new/annotations_SD_inc')\n",
    "parser.add_argument('--cp_checkpoint', type=str, default='checkpoints/IDA_MICCAI2021_checkpoints/inc_SC_eCBS/')\n",
    "\n",
    "# graph\n",
    "parser.add_argument('--gsu_cbs',        type=bool, default=True)\n",
    "parser.add_argument('--gsu_checkpoint', type=str,  default='checkpoints/g_checkpoints/d2g_ecbs_resnet18_11_SC_eCBS/d2g_ecbs_resnet18_11_SC_eCBS/epoch_train/checkpoint_D2F70_epoch.pth')\n",
    "parser.add_argument('--gsu_file_dir', type=str,  default='datasets/instruments18/')\n",
    "parser.add_argument('--gsu_img_dir', type=str,  default='left_frames')\n",
    "parser.add_argument('--gsu_w2v_loc', type=str,  default='datasets/surgicalscene_word2vec.hdf5')\n",
    "parser.add_argument('--gsu_feat', type=str,  default='resnet18_11_SC_CBS')\n",
    "\n",
    "#parser.add_argument('--head', type=int, default=8)\n",
    "#parser.add_argument('--warmup', type=int, default=10000)\n",
    "#parser.add_argument('--features_path_DA', type=str)\n",
    "#parser.add_argument('--annotation_folder_DA', type=str)\n",
    "args = parser.parse_args(args=[])\n",
    "\n",
    "\n",
    "print(args)\n",
    "   \n",
    "# Pipeline for image regions and text\n",
    "image_field = ImageDetectionsField(detections_path=args.cp_features_path, max_detections=6, load_in_tmp=False)  \n",
    "text_field = TextField(init_token='<bos>', eos_token='<eos>', lower=True, tokenize='spacy', remove_punctuation=True, nopoints=False)\n",
    "\n",
    "# Create the dataset \n",
    "dataset = COCO(image_field, text_field, args.cp_features_path, args.cp_annotation_folder, args.cp_annotation_folder, args.gsu_file_dir, args.gsu_img_dir, args.gsu_w2v_loc, args.gsu_feat)\n",
    "train_dataset, val_dataset = dataset.splits   \n",
    "print('train:', len(train_dataset))\n",
    "print('val:', len(val_dataset))\n",
    "    \n",
    "# caption model\n",
    "if not os.path.isfile('datasets/vocab_%s.pkl' % args.exp_name):\n",
    "    print(\"Building vocabulary\")\n",
    "    text_field.build_vocab(train_dataset, val_dataset, min_freq=2)  \n",
    "    pickle.dump(text_field.vocab, open('datasets/vocab_%s.pkl' % args.exp_name, 'wb'))\n",
    "else:\n",
    "    text_field.vocab = pickle.load(open('datasets/vocab_%s.pkl' % args.exp_name, 'rb'))\n",
    "\n",
    "print('vocabulary size is:', len(text_field.vocab))\n",
    "print(text_field.vocab.stoi)\n",
    "\n",
    "if args.cp_cbs == 'True':\n",
    "    from models.transformer import MemoryAugmentedEncoder_CBS\n",
    "    print(\"MemoryAugmentedEncoder_CBS\")\n",
    "    encoder = MemoryAugmentedEncoder_CBS(3, 0, attention_module=ScaledDotProductAttentionMemory, attention_module_kwargs={'m': args.m})\n",
    "else:\n",
    "    print(\"MemoryAugmentedEncoder\")\n",
    "    encoder = MemoryAugmentedEncoder(3, 0, attention_module=ScaledDotProductAttentionMemory, attention_module_kwargs={'m': args.m}) \n",
    "\n",
    "decoder = MeshedDecoder(len(text_field.vocab), 54, 3, text_field.vocab.stoi['<pad>'])\n",
    "caption_model = Transformer(text_field.vocab.stoi['<bos>'], encoder, decoder).to(device)\n",
    "\n",
    "if args.cp_cbs == 'True':\n",
    "    caption_model.encoder.get_new_kernels(0, args.cp_kernel_sizex, args.cp_kernel_sizey, args.cp_decay_epoch, args.cp_std_factor, args.cp_cbs_filter) \n",
    "            \n",
    "\n",
    "# graph model    \n",
    "graph_su_model = AGRNN(bias= True, bn=False, dropout=0.3, multi_attn=False, layer=1, diff_edge=False, use_cbs = args.gsu_cbs)\n",
    "if args.gsu_cbs:\n",
    "    graph_su_model.grnn1.gnn.apply_h_h_edge.get_new_kernels(0)\n",
    "\n",
    "# loading pre-trained model for graph and caption\n",
    "pretrained_model = torch.load(args.cp_checkpoint+('%s_best.pth' % args.exp_name))\n",
    "caption_model.load_state_dict(pretrained_model['state_dict']) \n",
    "pretrained_model = torch.load(args.gsu_checkpoint)\n",
    "graph_su_model.load_state_dict(pretrained_model['state_dict'])\n",
    "    \n",
    "# model\n",
    "model = mtl_model(caption_model, graph_su_model)\n",
    "model = model.to(device)\n",
    "    \n",
    "# dataset\n",
    "dict_dataset_val = val_dataset.image_dictionary({'image': image_field, 'text': RawField()})\n",
    "dataloader_val = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False, num_workers=args.workers)\n",
    "dict_dataloader_val = DataLoader(dict_dataset_val, batch_size=args.batch_size) # for caption with word GT class number\n",
    "\n",
    "\n",
    "''' 1.0 mutlitask model base evaluation ==========================================================================='''\n",
    "scores, g_total_acc, g_map_value, g_total_loss, g_ece, g_sce, g_tace, g_brier, g_uce = evaluate_metrics(model, device, dict_dataloader_val, text_field)\n",
    "print('Initial Graph SU: acc: %0.6f map: %0.6f loss: %0.6f, ece:%0.6f, sce:%0.6f, tace:%0.6f, brier:%.6f, uce:%.6f' %(g_total_acc, g_map_value, g_total_loss, g_ece, g_sce, g_tace, g_brier, g_uce))\n",
    "print(\"Initial Caption scores :\", scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temperature Scaling : Model Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelWithTemperature(nn.Module):\n",
    "    '''\n",
    "    Temperature scaling model for model\n",
    "    '''\n",
    "    def __init__(self, model):\n",
    "        super(ModelWithTemperature, self).__init__()\n",
    "        self.model = model\n",
    "        self.use_ts = False\n",
    "        self.graph_su_temperature = nn.Parameter(torch.ones(1) * 1.5)\n",
    "        self.caption_temperature = nn.Parameter(torch.ones(1) * 1.5)\n",
    "\n",
    "    def forward(self, detections, captions, batch_graph, batch_h_node_list, batch_obj_node_list, batch_h_h_e_list, batch_o_o_e_list, batch_h_o_e_list, feat, spatial_feat, word2vec):\n",
    "        \n",
    "        # if self.use_ts:\n",
    "            # g_su_temp = self.graph_su_temperature.unsqueeze(1).expand(interaction.size(0), interaction.size(1))\n",
    "            # self.model.caption.decoder.caption_ts = self.caption_temperature\n",
    "        \n",
    "        caption_output, interaction = self.model(detections, captions, batch_graph, batch_h_node_list, batch_obj_node_list, batch_h_h_e_list, batch_o_o_e_list, batch_h_o_e_list, feat, spatial_feat, word2vec)\n",
    "        \n",
    "        # if self.use_ts:\n",
    "            # interaction = interaction / g_su_temp\n",
    "\n",
    "        return caption_output, interaction\n",
    "\n",
    "    def graph_su_set_temperature(self, valid_loader):\n",
    "        '''\n",
    "        Finding optimal temperature scale for graph scene understanding task\n",
    "        '''\n",
    "        self.cuda()\n",
    "        g_logits_list = []\n",
    "        g_labels_list = []\n",
    "        g_criterion = nn.MultiLabelSoftMarginLoss()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for it, (images, caps_gt, graph_data) in enumerate(iter(valid_loader)):\n",
    "                # graph\n",
    "                img_name = graph_data['img_name']\n",
    "                img_loc = graph_data['img_loc']\n",
    "                node_num = graph_data['node_num']\n",
    "                roi_labels = graph_data['roi_labels']\n",
    "                det_boxes = graph_data['det_boxes']\n",
    "                edge_labels = graph_data['edge_labels']\n",
    "                edge_num = graph_data['edge_num']\n",
    "                features = graph_data['features']\n",
    "                spatial_feat = graph_data['spatial_feat']\n",
    "                word2vec = graph_data['word2vec']\n",
    "                features, spatial_feat, word2vec, edge_labels = features.to(device), spatial_feat.to(device), word2vec.to(device), edge_labels.to(device)    \n",
    "        \n",
    "                g_output = self.model.graph_su(node_num, features, spatial_feat, word2vec, roi_labels, validation=True)\n",
    "                    \n",
    "                g_logits_list.append(g_output)\n",
    "                g_labels_list.append(edge_labels)\n",
    "                \n",
    "            g_logits = torch.cat(g_logits_list).cuda()\n",
    "            g_labels = torch.cat(g_labels_list).cuda()\n",
    "\n",
    "        #init_temp = self.graph_su_temperature.clone()\n",
    "        optimizer = optim.LBFGS([self.graph_su_temperature], lr=0.01, max_iter=50)\n",
    "\n",
    "        def eval():\n",
    "            g_su_temp = self.graph_su_temperature.unsqueeze(1).expand(g_logits.size(0), g_logits.size(1))\n",
    "            g_logit_out = F.softmax(g_logits/g_su_temp, dim=1)\n",
    "            g_loss = g_criterion(g_logit_out, g_labels.float())\n",
    "            g_loss.backward()\n",
    "            return g_loss\n",
    "        \n",
    "        optimizer.step(eval)\n",
    "        return\n",
    "\n",
    "    def caption_set_temperature(self, valid_loader):\n",
    "        '''\n",
    "        Finding optimal temperature scale for caption task\n",
    "        '''\n",
    "        self.cuda()\n",
    "        c_logits_list = None\n",
    "        c_labels_list = None\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for it, (images, caps_gt) in enumerate(iter(valid_loader)):    \n",
    "                images, caps_gt = images.to(device), caps_gt.to(device)\n",
    "                caption_out = self.model.caption(images, caps_gt)\n",
    "\n",
    "                if c_logits_list is not None:\n",
    "                    c_logits_list = torch.cat([c_logits_list, caption_out], 1)\n",
    "                    c_labels_list = torch.cat([c_labels_list, caps_gt],1)\n",
    "                else:\n",
    "                    c_logits_list = caption_out\n",
    "                    c_labels_list = caps_gt\n",
    "\n",
    "            c_logits = c_logits_list.cuda()\n",
    "            c_labels = c_labels_list.cuda()\n",
    "        \n",
    "        init_temp = self.caption_temperature.clone()\n",
    "        optimizer = optim.LBFGS([self.caption_temperature], lr=0.01, max_iter=50)\n",
    "\n",
    "        def eval():\n",
    "            caption_temp = self.caption_temperature.unsqueeze(1).expand(c_logits.size(1), c_logits.size(0))\n",
    "            c_criterion = CELossWithLS(classes=len(text_field.vocab), smoothing=0.1, gamma=0.0, isCos=False, ignore_index=text_field.vocab.stoi['<pad>'])\n",
    "            c_base = c_logits/caption_temp\n",
    "            c_loss = c_criterion(c_base[:, :-1].contiguous(), c_labels[:, 1:].contiguous())\n",
    "            c_loss.backward()\n",
    "            return c_loss\n",
    "        \n",
    "        optimizer.step(eval)\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temperature Scaling : ECE Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _ECELoss(nn.Module):\n",
    "    '''\n",
    "    Expected Calibration Error for Calibration\n",
    "    '''\n",
    "    def __init__(self, n_bins=15):\n",
    "        \"\"\"\n",
    "        n_bins (int): number of confidence interval bins\n",
    "        \"\"\"\n",
    "        super(_ECELoss, self).__init__()\n",
    "        bin_boundaries = torch.linspace(0, 1, n_bins + 1)\n",
    "        self.bin_lowers = bin_boundaries[:-1]\n",
    "        self.bin_uppers = bin_boundaries[1:]\n",
    "\n",
    "    def forward(self, logits, labels):\n",
    "        if labels.size(1) == 13:\n",
    "            gt_labels = torch.argmax(labels, dim=1)\n",
    "        else: \n",
    "            logits = logits.squeeze()\n",
    "            gt_labels = labels.squeeze()\n",
    "\n",
    "        softmaxes = F.softmax(logits, dim=1)\n",
    "        confidences, predictions = torch.max(softmaxes, 1)\n",
    "        accuracies = predictions.eq(gt_labels)\n",
    "        ece = torch.zeros(1, device=logits.device)\n",
    "        for bin_lower, bin_upper in zip(self.bin_lowers, self.bin_uppers):\n",
    "            in_bin = confidences.gt(bin_lower.item()) * confidences.le(bin_upper.item())\n",
    "            prop_in_bin = in_bin.float().mean()\n",
    "            if prop_in_bin.item() > 0:\n",
    "                accuracy_in_bin = accuracies[in_bin].float().mean()\n",
    "                avg_confidence_in_bin = confidences[in_bin].mean()\n",
    "                ece += torch.abs(avg_confidence_in_bin - accuracy_in_bin) * prop_in_bin\n",
    "        return ece"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temperature Scaling : Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ts_evaluation(model_ts, g_c_val_dataloader, c_val_dataloader, g_temp=1.5, c_temp=1):\n",
    "    '''\n",
    "    '''\n",
    "    model.eval()\n",
    "\n",
    "    g_acc = 0.0\n",
    "    c_acc = 0.0\n",
    "    g_loss = 0.0\n",
    "    c_loss = 0.0\n",
    "    g_logits = []\n",
    "    g_labels = []\n",
    "    c_logits = []\n",
    "    c_labels = []\n",
    "    \n",
    "    g_logits_list = []\n",
    "    g_labels_list = []\n",
    "    c_logits_list = None\n",
    "    c_labels_list = None\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for it, (images, caps_gt, graph_data) in enumerate(iter(g_c_val_dataloader)):\n",
    "            # graph\n",
    "            img_name = graph_data['img_name']\n",
    "            img_loc = graph_data['img_loc']\n",
    "            node_num = graph_data['node_num']\n",
    "            roi_labels = graph_data['roi_labels']\n",
    "            det_boxes = graph_data['det_boxes']\n",
    "            edge_labels = graph_data['edge_labels']\n",
    "            edge_num = graph_data['edge_num']\n",
    "            features = graph_data['features']\n",
    "            spatial_feat = graph_data['spatial_feat']\n",
    "            word2vec = graph_data['word2vec']\n",
    "            features, spatial_feat, word2vec, edge_labels = features.to(device), spatial_feat.to(device), word2vec.to(device), edge_labels.to(device)    \n",
    "        \n",
    "            g_output = model_ts.model.graph_su(node_num, features, spatial_feat, word2vec, roi_labels, validation=True)\n",
    "            \n",
    "            g_output = g_output / g_temp\n",
    "            g_logits_list.append(g_output)\n",
    "            g_labels_list.append(edge_labels)\n",
    "                \n",
    "    g_logits = torch.cat(g_logits_list).cuda()\n",
    "    g_labels = torch.cat(g_labels_list).cuda()\n",
    "\n",
    "    g_logits = F.softmax(g_logits, dim=1)\n",
    "    g_criterion = nn.MultiLabelSoftMarginLoss()\n",
    "    g_loss = g_criterion(g_logits, g_labels.float())\n",
    "    g_acc = np.sum(np.equal(np.argmax(g_logits.cpu().data.numpy(), axis=-1), np.argmax(g_labels.cpu().data.numpy(), axis=-1))) / g_labels.size(0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for it, (images, caps_gt) in enumerate(iter(c_val_dataloader)):    \n",
    "            images, caps_gt = images.to(device), caps_gt.to(device)\n",
    "            caption_out = model_ts.model.caption(images, caps_gt)\n",
    "            caption_out = caption_out/c_temp\n",
    "            if c_logits_list is not None:\n",
    "                c_logits_list = torch.cat([c_logits_list, caption_out], 1)\n",
    "                c_labels_list = torch.cat([c_labels_list, caps_gt],1)\n",
    "            else:\n",
    "                c_logits_list = caption_out\n",
    "                c_labels_list = caps_gt\n",
    "\n",
    "    c_logits = c_logits_list.cuda()\n",
    "    c_labels = c_labels_list.cuda()\n",
    "    \n",
    "    c_criterion = CELossWithLS(classes=len(text_field.vocab), smoothing=0.1, gamma=0.0, isCos=False, ignore_index=text_field.vocab.stoi['<pad>'])\n",
    "    c_loss = c_criterion(c_logits[:, :-1].contiguous(), c_labels[:, 1:].contiguous())\n",
    "    c_acc = np.sum(np.equal(np.argmax(c_logits.cpu().data.numpy(), axis=-1), c_labels.cpu().data.numpy())) / c_labels.size(1)\n",
    "    \n",
    "    return (g_loss.item(), c_loss.item(), g_acc, c_acc, g_logits, g_labels, c_logits, c_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.0: Temperature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Graph SU Temperature:1.5000\n",
      "Initial Caption Temperature:1.5000\n"
     ]
    }
   ],
   "source": [
    "model_ts = ModelWithTemperature(model)\n",
    "ece_criterion = _ECELoss().to(device)\n",
    "print('Initial Graph SU Temperature:%.4f'%model_ts.graph_su_temperature.item())\n",
    "print('Initial Caption Temperature:%0.4f'%model_ts.caption_temperature.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1: Temperature Scaling : Find Optimal value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------\n",
      "Optimal Graph SU Temperature:1.3954\n",
      "Optimal Caption Temperature:4.8063\n"
     ]
    }
   ],
   "source": [
    "model_ts.graph_su_set_temperature(dict_dataloader_val)\n",
    "model_ts.caption_set_temperature(dataloader_val)\n",
    "print('-----------------------------------------------------------------------')\n",
    "print('Optimal Graph SU Temperature:%.4f'%model_ts.graph_su_temperature.item())\n",
    "print('Optimal Caption Temperature:%0.4f'%model_ts.caption_temperature.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Temperature Scaling : Without TS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "evaluation:   0%|          | 0/447 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------\n",
      "Before TS: G_loss:0.710, G_acc:0.571, G_ece:0.46277\n",
      "Before TS: C_loss:2.724, C_acc:0.022, C_ece:0.84285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluation: 100%|██████████| 447/447 [01:56<00:00,  3.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.571059 map: 0.321686 loss: 0.509024, ece:0.219497, sce:0.047773, tace:0.049699, brier:0.649657, uce:0.283210\n",
      "Before TS: Graph SU: acc: 0.571059 map: 0.321686 loss: 0.509024, ece:0.219497, sce:0.047773, tace:0.049699, brier:0.649657, uce:0.283210\n",
      "Before TS: Caption scores : {'BLEU': array([0.5498, 0.4714, 0.4238, 0.3801]), 'METEOR': 0.2861, 'ROUGE': 0.57, 'CIDEr': 2.7487}\n"
     ]
    }
   ],
   "source": [
    "g_loss, c_loss, g_acc, c_acc, g_logits, g_labels, c_logits, c_labels = ts_evaluation(model_ts, dict_dataloader_val, \n",
    "                                    dataloader_val)\n",
    "g_temperature_ece = ece_criterion(g_logits, g_labels).item()\n",
    "c_temperature_ece = ece_criterion(c_logits, c_labels).item()\n",
    "print('-----------------------------------------------------------------------')\n",
    "print('Before TS: G_loss:%.3f, G_acc:%.3f, G_ece:%.5f'%(g_loss, g_acc, g_temperature_ece))\n",
    "print('Before TS: C_loss:%.3f, C_acc:%.3f, C_ece:%.5f'%(c_loss, c_acc, c_temperature_ece))\n",
    "\n",
    "scores, g_total_acc, g_map_value, g_total_loss, g_ece, g_sce, g_tace, g_brier, g_uce = evaluate_metrics(model_ts.model, device, dict_dataloader_val, text_field)\n",
    "print('Before TS: Graph SU: acc: %0.6f map: %0.6f loss: %0.6f, ece:%0.6f, sce:%0.6f, tace:%0.6f, brier:%.6f, uce:%.6f' %(g_total_acc, g_map_value, g_total_loss, g_ece, g_sce, g_tace, g_brier, g_uce))\n",
    "print(\"Before TS: Caption scores :\", scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3 Temperature Scaling : With TS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "evaluation:   0%|          | 0/447 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------\n",
      "Optimal TS: G_loss:0.709, G_acc:0.571, G_ece:0.46025\n",
      "Optimal TS: C_loss:1.894, C_acc:0.022, C_ece:0.29004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluation: 100%|██████████| 447/447 [01:57<00:00,  3.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.571059 map: 0.315094 loss: 0.487805, ece:0.218120, sce:0.045566, tace:0.046718, brier:0.644169, uce:0.260813\n",
      "Optimal TS: Graph SU: acc: 0.571059 map: 0.315094 loss: 0.487805, ece:0.218120, sce:0.045566, tace:0.046718, brier:0.644169, uce:0.260813\n",
      "Optimal TS: Caption scores : {'BLEU': array([0.3623, 0.3231, 0.3032, 0.2864]), 'METEOR': 0.2151, 'ROUGE': 0.4628, 'CIDEr': 2.3378}\n"
     ]
    }
   ],
   "source": [
    "g_loss, c_loss, g_acc, c_acc, g_logits, g_labels, c_logits, c_labels = ts_evaluation(model_ts, dict_dataloader_val, \n",
    "                                    dataloader_val, g_temp=model_ts.graph_su_temperature.item(), c_temp=model_ts.caption_temperature.item())\n",
    "g_temperature_ece = ece_criterion(g_logits, g_labels).item()\n",
    "c_temperature_ece = ece_criterion(c_logits, c_labels).item()\n",
    "print('-----------------------------------------------------------------------')\n",
    "print('Optimal TS: G_loss:%.3f, G_acc:%.3f, G_ece:%.5f'%(g_loss, g_acc, g_temperature_ece))\n",
    "print('Optimal TS: C_loss:%.3f, C_acc:%.3f, C_ece:%.5f'%(c_loss, c_acc, c_temperature_ece))\n",
    "\n",
    "scores, g_total_acc, g_map_value, g_total_loss, g_ece, g_sce, g_tace, g_brier, g_uce = evaluate_metrics(model_ts.model, device, dict_dataloader_val, text_field, g_temp = model_ts.graph_su_temperature.item(), c_temp = model_ts.caption_temperature.item())\n",
    "print('Optimal TS: Graph SU: acc: %0.6f map: %0.6f loss: %0.6f, ece:%0.6f, sce:%0.6f, tace:%0.6f, brier:%.6f, uce:%.6f' %(g_total_acc, g_map_value, g_total_loss, g_ece, g_sce, g_tace, g_brier, g_uce))\n",
    "print(\"Optimal TS: Caption scores :\", scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.4 Temperature Scaling : With CDA-TS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "evaluation:   0%|          | 0/447 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------\n",
      "CDA-TS: G_loss:0.709, G_acc:0.574, G_ece:0.46348\n",
      "CDA-TS: C_loss:3.262, C_acc:0.022, C_ece:0.02099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluation: 100%|██████████| 447/447 [01:56<00:00,  3.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.573643 map: 0.319211 loss: 0.489725, ece:0.217880, sce:0.045660, tace:0.046676, brier:0.643576, uce:0.267471\n",
      "CDA-TS: Graph SU: acc: 0.573643 map: 0.319211 loss: 0.489725, ece:0.217880, sce:0.045660, tace:0.046676, brier:0.643576, uce:0.267471\n",
      "CDA-TS: Caption scores : {'BLEU': array([0.3623, 0.3231, 0.3032, 0.2864]), 'METEOR': 0.2151, 'ROUGE': 0.4628, 'CIDEr': 2.3378}\n"
     ]
    }
   ],
   "source": [
    "g_cls_freq = torch.zeros(13)\n",
    "for i in torch.argmax(g_labels, dim=1): g_cls_freq[i] += 1\n",
    "g_cls_freq_norm = g_cls_freq/torch.max(g_cls_freq)\n",
    "g_temp = model_ts.graph_su_temperature.item() + g_cls_freq_norm*0.1\n",
    "#g_temp = g_temp.to(device)\n",
    "model_ts.graph_su_temperature = nn.Parameter(g_temp.to(device))\n",
    "\n",
    "c_cls_freq = torch.zeros(41)\n",
    "for i in c_labels.squeeze(): c_cls_freq[i] += 1\n",
    "c_cls_freq_norm = c_cls_freq/torch.max(c_cls_freq)\n",
    "c_temp = model_ts.caption_temperature.item() + c_cls_freq_norm*0.1\n",
    "#c_temp = c_temp.to(device)\n",
    "model_ts.caption_temperature = nn.Parameter(c_temp.to(device))\n",
    "\n",
    "g_loss, c_loss, g_acc, c_acc, g_logits, g_labels, c_logits, c_labels = ts_evaluation(model_ts, dict_dataloader_val, \n",
    "                                    dataloader_val, g_temp = model_ts.graph_su_temperature, c_temp = model_ts.caption_temperature)\n",
    "g_temperature_ece = ece_criterion(g_logits, g_labels).item()\n",
    "c_temperature_ece = ece_criterion(c_logits, c_labels).item()\n",
    "print('-----------------------------------------------------------------------')\n",
    "print('CDA-TS: G_loss:%.3f, G_acc:%.3f, G_ece:%.5f'%(g_loss, g_acc, g_temperature_ece))\n",
    "print('CDA-TS: C_loss:%.3f, C_acc:%.3f, C_ece:%.5f'%(c_loss, c_acc, c_temperature_ece))\n",
    "\n",
    "scores, g_total_acc, g_map_value, g_total_loss, g_ece, g_sce, g_tace, g_brier, g_uce = evaluate_metrics(model_ts.model, device, dict_dataloader_val, text_field, g_temp = model_ts.graph_su_temperature, c_temp = model_ts.caption_temperature)\n",
    "print('CDA-TS: Graph SU: acc: %0.6f map: %0.6f loss: %0.6f, ece:%0.6f, sce:%0.6f, tace:%0.6f, brier:%.6f, uce:%.6f' %(g_total_acc, g_map_value, g_total_loss, g_ece, g_sce, g_tace, g_brier, g_uce))\n",
    "print(\"CDA-TS: Caption scores :\", scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confidence Aware distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conf_dist(model_ts, g_c_val_dataloader, c_val_dataloader):\n",
    "\n",
    "    g_conf_list = np.zeros(13)\n",
    "    c_conf_list = np.zeros(41)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for it, (images, caps_gt, graph_data) in enumerate(iter(g_c_val_dataloader)):\n",
    "            # graph\n",
    "            img_name = graph_data['img_name']\n",
    "            img_loc = graph_data['img_loc']\n",
    "            node_num = graph_data['node_num']\n",
    "            roi_labels = graph_data['roi_labels']\n",
    "            det_boxes = graph_data['det_boxes']\n",
    "            edge_labels = graph_data['edge_labels']\n",
    "            edge_num = graph_data['edge_num']\n",
    "            features = graph_data['features']\n",
    "            spatial_feat = graph_data['spatial_feat']\n",
    "            word2vec = graph_data['word2vec']\n",
    "            features, spatial_feat, word2vec, edge_labels = features.to(device), spatial_feat.to(device), word2vec.to(device), edge_labels.to(device)    \n",
    "        \n",
    "            g_output = model_ts.model.graph_su(node_num, features, spatial_feat, word2vec, roi_labels, validation=True)\n",
    "            \n",
    "            g_output = F.softmax(g_output,1)\n",
    "            g_confidences, g_predictions = torch.max(g_output, 1)\n",
    "            g_accuracies = g_predictions.eq(torch.argmax(edge_labels, dim=1))\n",
    "            for i in torch.argmax(edge_labels, dim=1).unique():\n",
    "                g_conf_list[i] += g_confidences[torch.argmax(edge_labels, dim=1)==i].sum()\n",
    "\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for it, (images, caps_gt) in enumerate(iter(c_val_dataloader)):    \n",
    "            images, caps_gt = images.to(device), caps_gt.to(device)\n",
    "            c_output = model_ts.model.caption(images, caps_gt)\n",
    "            c_output = c_output.squeeze()\n",
    "            c_output = F.softmax(c_output,1)\n",
    "            c_confidences, c_predictions = torch.max(c_output, 1)\n",
    "            c_accuracies = c_predictions.eq(caps_gt.squeeze())\n",
    "            for i in caps_gt.squeeze().unique():\n",
    "                c_conf_list[i] += c_confidences[caps_gt.squeeze()==i].sum()\n",
    "\n",
    "    return g_conf_list, c_conf_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.5 Temperature Scaling : With CCA-TS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_conf_list, c_conf_list = conf_dist(model_ts, dict_dataloader_val, dataloader_val)\n",
    "\n",
    "plt.figure(1)\n",
    "plt.title('Graph SU class distribution')\n",
    "plt.bar(np.arange(len(g_cls_freq)),g_cls_freq)\n",
    "plt.savefig('graph_class_dist.png')\n",
    "\n",
    "plt.figure(2)\n",
    "plt.title('Graph SU confidence score distribution')\n",
    "plt.bar(np.arange(len(g_cls_freq)),g_conf_list/12)\n",
    "plt.savefig('graph_conf_dist.png')\n",
    "    \n",
    "plt.figure(3)\n",
    "plt.title('Caption Class Distribution')\n",
    "plt.bar(np.arange(len(c_cls_freq)),c_cls_freq)\n",
    "plt.savefig('caption_cls_dist.png')\n",
    "    \n",
    "plt.figure(4)\n",
    "plt.title('Caption Confidence distribution')\n",
    "plt.bar(np.arange(len(c_cls_freq)),c_conf_list/41)\n",
    "plt.savefig('caption_conf_dist.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CCA-TS calculation V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "evaluation:   0%|          | 0/447 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------\n",
      "CCA-TS-V1: G_loss:0.709, G_acc:0.575, G_ece:0.46582\n",
      "CCA-TS-V1: C_loss:3.272, C_acc:0.022, C_ece:0.02043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluation: 100%|██████████| 447/447 [01:56<00:00,  3.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.575366 map: 0.320200 loss: 0.491720, ece:0.216305, sce:0.045280, tace:0.046805, brier:0.643167, uce:0.266481\n",
      "CCA-TS-V1: Graph SU: acc: 0.575366 map: 0.320200 loss: 0.491720, ece:0.216305, sce:0.045280, tace:0.046805, brier:0.643167, uce:0.266481\n",
      "CCA-TS-V1: Caption scores : {'BLEU': array([0.3623, 0.3231, 0.3032, 0.2864]), 'METEOR': 0.2151, 'ROUGE': 0.4628, 'CIDEr': 2.3378}\n"
     ]
    }
   ],
   "source": [
    "g_cls_freq = g_conf_list/13\n",
    "g_cls_freq_norm = g_cls_freq/np.max(g_cls_freq)\n",
    "g_cls_freq_norm = torch.tensor(g_cls_freq_norm).float()\n",
    "g_temp = model_ts.graph_su_temperature.cpu() + g_cls_freq_norm*0.1\n",
    "# g_temp = g_temp.to(device)\n",
    "model_ts.graph_su_temperature = nn.Parameter(g_temp.to(device))\n",
    "    \n",
    "\n",
    "c_cls_freq = c_conf_list/41#train_dataset.get_cls_num_list()\n",
    "c_cls_freq_norm = c_cls_freq/np.max(c_cls_freq)\n",
    "c_cls_freq_norm = torch.tensor(c_cls_freq_norm).float()\n",
    "c_temp =  model_ts.caption_temperature.cpu() + c_cls_freq_norm*0.1\n",
    "# c_temp = c_temp.to(device)\n",
    "model_ts.caption_temperature = nn.Parameter(c_temp.to(device))\n",
    "    \n",
    "g_loss, c_loss, g_acc, c_acc, g_logits, g_labels, c_logits, c_labels = ts_evaluation(model_ts, dict_dataloader_val, \n",
    "                                    dataloader_val, g_temp = model_ts.graph_su_temperature, c_temp = model_ts.caption_temperature)\n",
    "g_temperature_ece = ece_criterion(g_logits, g_labels).item()\n",
    "c_temperature_ece = ece_criterion(c_logits, c_labels).item()\n",
    "print('-----------------------------------------------------------------------')\n",
    "print('CCA-TS-V1: G_loss:%.3f, G_acc:%.3f, G_ece:%.5f'%(g_loss, g_acc, g_temperature_ece))\n",
    "print('CCA-TS-V1: C_loss:%.3f, C_acc:%.3f, C_ece:%.5f'%(c_loss, c_acc, c_temperature_ece))\n",
    "\n",
    "scores, g_total_acc, g_map_value, g_total_loss, g_ece, g_sce, g_tace, g_brier, g_uce = evaluate_metrics(model_ts.model, device, dict_dataloader_val, text_field, g_temp = model_ts.graph_su_temperature, c_temp = model_ts.caption_temperature)\n",
    "print('CCA-TS-V1: Graph SU: acc: %0.6f map: %0.6f loss: %0.6f, ece:%0.6f, sce:%0.6f, tace:%0.6f, brier:%.6f, uce:%.6f' %(g_total_acc, g_map_value, g_total_loss, g_ece, g_sce, g_tace, g_brier, g_uce))\n",
    "print(\"CCA-TS-V1: Caption scores :\", scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CCA-TS calculation V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "evaluation:   0%|          | 0/447 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------\n",
      "CCA-TS-V2: G_loss:0.709, G_acc:0.575, G_ece:0.46582\n",
      "CCA-TS-V2: C_loss:3.283, C_acc:0.022, C_ece:0.01962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluation: 100%|██████████| 447/447 [01:55<00:00,  3.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.575366 map: 0.320200 loss: 0.491720, ece:0.216305, sce:0.045280, tace:0.046805, brier:0.643167, uce:0.266481\n",
      "CCA-TS-V2: Graph SU: acc: 0.575366 map: 0.320200 loss: 0.491720, ece:0.216305, sce:0.045280, tace:0.046805, brier:0.643167, uce:0.266481\n",
      "CCA-TS-V2: Caption scores : {'BLEU': array([0.3623, 0.3231, 0.3032, 0.2864]), 'METEOR': 0.2151, 'ROUGE': 0.4628, 'CIDEr': 2.3378}\n"
     ]
    }
   ],
   "source": [
    "g_cls_freq = g_conf_list/13\n",
    "g_cls_freq_norm = g_cls_freq/np.max(g_cls_freq)\n",
    "g_cls_freq_norm = torch.tensor(g_cls_freq_norm).float()\n",
    "g_temp = model_ts.graph_su_temperature.cpu() + (g_cls_freq_norm.exp()-1.0)*.1\n",
    "# g_temp = g_temp.to(device)\n",
    "model_ts.graph_su_temperature.i = nn.Parameter(g_temp.to(device))\n",
    "    \n",
    "c_cls_freq = c_conf_list/41#train_dataset.get_cls_num_list()\n",
    "c_cls_freq_norm = c_cls_freq/np.max(c_cls_freq)\n",
    "c_cls_freq_norm = torch.tensor(c_cls_freq_norm).float()\n",
    "c_temp =  model_ts.caption_temperature.cpu() + (c_cls_freq_norm.exp()-1.0)*.1\n",
    "# c_temp = c_temp.to(device)\n",
    "model_ts.caption_temperature = nn.Parameter(c_temp.to(device))\n",
    "    \n",
    "g_loss, c_loss, g_acc, c_acc, g_logits, g_labels, c_logits, c_labels = ts_evaluation(model_ts, dict_dataloader_val, \n",
    "                                    dataloader_val, g_temp = model_ts.graph_su_temperature, c_temp = model_ts.caption_temperature)\n",
    "g_temperature_ece = ece_criterion(g_logits, g_labels).item()\n",
    "c_temperature_ece = ece_criterion(c_logits, c_labels).item()\n",
    "print('-----------------------------------------------------------------------')\n",
    "print('CCA-TS-V2: G_loss:%.3f, G_acc:%.3f, G_ece:%.5f'%(g_loss, g_acc, g_temperature_ece))\n",
    "print('CCA-TS-V2: C_loss:%.3f, C_acc:%.3f, C_ece:%.5f'%(c_loss, c_acc, c_temperature_ece))\n",
    "\n",
    "scores, g_total_acc, g_map_value, g_total_loss, g_ece, g_sce, g_tace, g_brier, g_uce = evaluate_metrics(model_ts.model, device, dict_dataloader_val, text_field, g_temp = model_ts.graph_su_temperature, c_temp = model_ts.caption_temperature)\n",
    "print('CCA-TS-V2: Graph SU: acc: %0.6f map: %0.6f loss: %0.6f, ece:%0.6f, sce:%0.6f, tace:%0.6f, brier:%.6f, uce:%.6f' %(g_total_acc, g_map_value, g_total_loss, g_ece, g_sce, g_tace, g_brier, g_uce))\n",
    "print(\"CCA-TS-V2: Caption scores :\", scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
