{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import random\n",
    "\n",
    "import json\n",
    "import h5py\n",
    "import itertools\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import argparse, pickle\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from torch import optim\n",
    "import torchvision.models\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "from torch.utils.data import DataLoader as TorchDataLoader\n",
    "\n",
    "# caption libraries\n",
    "import evaluation\n",
    "import collections\n",
    "from data.utils import nostdout\n",
    "from data.example import Example\n",
    "from data.field import TextField, RawField\n",
    "from models.transformer import MemoryAugmentedEncoder_CBS\n",
    "from models.transformer import Transformer, MemoryAugmentedEncoder, MeshedDecoder, ScaledDotProductAttentionMemory\n",
    "\n",
    "# graph libraries\n",
    "from utils.g_vis_img import *\n",
    "from models.graph_su import *\n",
    "from evaluation.graph_eval import *\n",
    "\n",
    "# feature extractor\n",
    "from models.feature_extractor import *\n",
    "\n",
    "# Random seeds\n",
    "seed = 27\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SurgicalSceneConstants():\n",
    "    '''\n",
    "    Surgical Scene constants\n",
    "    '''\n",
    "    def __init__( self):\n",
    "        self.instrument_classes = ( 'kidney', 'bipolar_forceps', 'prograsp_forceps', 'large_needle_driver',\n",
    "                                'monopolar_curved_scissors', 'ultrasound_probe', 'suction', 'clip_applier',\n",
    "                                'stapler', 'maryland_dissector', 'spatulated_monopolar_cautery')\n",
    "        self.action_classes = ( 'Idle', 'Grasping', 'Retraction', 'Tissue_Manipulation', \n",
    "                                'Tool_Manipulation', 'Cutting', 'Cauterization', \n",
    "                                'Suction', 'Looping', 'Suturing', 'Clipping', 'Staple', \n",
    "                                'Ultrasound_Sensing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-entropy loss with label smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CELossWithLS(torch.nn.Module):\n",
    "    '''\n",
    "    label smoothing cross-entropy loss for captioning\n",
    "    '''\n",
    "    def __init__(self, classes=None, smoothing=0.1, gamma=3.0, isCos=True, ignore_index=-1):\n",
    "        super(CELossWithLS, self).__init__()\n",
    "        self.complement = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.cls = classes\n",
    "        self.log_softmax = torch.nn.LogSoftmax(dim=1)\n",
    "        self.gamma = gamma\n",
    "        self.ignore_index = ignore_index\n",
    "\n",
    "    def forward(self, logits, target):\n",
    "        with torch.no_grad():\n",
    "            oh_labels = F.one_hot(target.to(torch.int64), num_classes = self.cls).permute(0,1,2).contiguous()\n",
    "            smoothen_ohlabel = oh_labels * self.complement + self.smoothing / self.cls\n",
    "\n",
    "        logs = self.log_softmax(logits[target!=self.ignore_index])\n",
    "        pt = torch.exp(logs)\n",
    "        return -torch.sum((1-pt).pow(self.gamma)*logs * smoothen_ohlabel[target!=self.ignore_index], dim=1).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader(TorchDataLoader):\n",
    "    '''\n",
    "    Custom dataloader\n",
    "    '''\n",
    "    def __init__(self, dataset, *args, **kwargs):\n",
    "        super(DataLoader, self).__init__(dataset, *args, collate_fn=dataset.collate_fn(), **kwargs)\n",
    "\n",
    "class Dataset(object):\n",
    "    '''\n",
    "    Custom Dataset to process dataset for both graph scene understanding and caption generation.\n",
    "    '''\n",
    "    def __init__(self, examples, fields, gsu_const):\n",
    "        self.examples = examples\n",
    "        self.fields = dict(fields)\n",
    "        self.file_dir = gsu_const['file_dir']\n",
    "        self.img_dir = gsu_const['img_dir']\n",
    "        self.dataconst = gsu_const['dataconst']\n",
    "        self.feature_extractor = gsu_const['feature_extractor']\n",
    "        self.word2vec = h5py.File(gsu_const['w2v_loc'], 'r')\n",
    "        \n",
    "    # word2vec\n",
    "    def _get_word2vec(self,node_ids):\n",
    "        word2vec = np.empty((0,300))\n",
    "        for node_id in node_ids:\n",
    "            vec = self.word2vec[self.dataconst.instrument_classes[node_id]]\n",
    "            word2vec = np.vstack((word2vec, vec))\n",
    "        return word2vec\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        example = self.examples[i]\n",
    "        frame_path = getattr(example, 'image')\n",
    "        frame_path = frame_path.split(\"/\")\n",
    "        _img_loc = os.path.join(self.file_dir, frame_path[0],self.img_dir,frame_path[-1].split(\"_\")[0]+'.png')\n",
    "        frame_data = h5py.File(os.path.join(self.file_dir, frame_path[0],'vsgat',self.feature_extractor, frame_path[-1].split(\"_\")[0]+'_features.hdf5'), 'r')    \n",
    "\n",
    "        # caption data\n",
    "        cp_data = []\n",
    "        for field_name, field in self.fields.items():\n",
    "            if field_name == 'image' and field == None: cp_data.append(np.zeros((6,512), dtype = np.float32))\n",
    "            else: cp_data.append(field.preprocess(getattr(example, field_name)))   \n",
    "        if len(cp_data) == 1: cp_data = cp_data[0]\n",
    "        \n",
    "        # graph data\n",
    "        gsu_data = {}\n",
    "        gsu_data['img_name'] = frame_data['img_name'].value[:] + '.jpg'\n",
    "        gsu_data['img_loc'] = _img_loc\n",
    "        gsu_data['node_num'] = frame_data['node_num'].value\n",
    "        gsu_data['roi_labels'] = frame_data['classes'][:]\n",
    "        gsu_data['det_boxes'] = frame_data['boxes'][:]\n",
    "        gsu_data['edge_labels'] = frame_data['edge_labels'][:]\n",
    "        gsu_data['edge_num'] = gsu_data['edge_labels'].shape[0]\n",
    "        gsu_data['spatial_feat'] = frame_data['spatial_features'][:]\n",
    "        gsu_data['word2vec'] = self._get_word2vec(gsu_data['roi_labels'])\n",
    "        if self.fields['image'] == None: gsu_data['features'] = np.zeros((gsu_data['node_num'],512), dtype = np.float32)\n",
    "        else: gsu_data['features'] = frame_data['node_features'][:]\n",
    "        \n",
    "        data = {}\n",
    "        data['cp_data'] = cp_data\n",
    "        data['gsu_data'] = gsu_data\n",
    "        return data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getattr__(self, attr):\n",
    "        if attr in self.fields:\n",
    "            for x in self.examples:\n",
    "                yield getattr(x, attr)\n",
    "                \n",
    "    def collate_fn(self):\n",
    "        def collate(batch):\n",
    "            gsu_batch_data = {}\n",
    "            gsu_batch_data['img_name'] = []\n",
    "            gsu_batch_data['img_loc'] = []\n",
    "            gsu_batch_data['node_num'] = []\n",
    "            gsu_batch_data['roi_labels'] = []\n",
    "            gsu_batch_data['det_boxes'] = []\n",
    "            gsu_batch_data['edge_labels'] = []\n",
    "            gsu_batch_data['edge_num'] = []\n",
    "            gsu_batch_data['features'] = []\n",
    "            gsu_batch_data['spatial_feat'] = []\n",
    "            gsu_batch_data['word2vec'] = []\n",
    "\n",
    "            for data in batch:\n",
    "                gsu_batch_data['img_name'].append(data['gsu_data']['img_name'])\n",
    "                gsu_batch_data['img_loc'].append(data['gsu_data']['img_loc'])\n",
    "                gsu_batch_data['node_num'].append(data['gsu_data']['node_num'])\n",
    "                gsu_batch_data['roi_labels'].append(data['gsu_data']['roi_labels'])\n",
    "                gsu_batch_data['det_boxes'].append(data['gsu_data']['det_boxes'])\n",
    "                gsu_batch_data['edge_labels'].append(data['gsu_data']['edge_labels'])\n",
    "                gsu_batch_data['edge_num'].append(data['gsu_data']['edge_num'])\n",
    "                gsu_batch_data['features'].append(data['gsu_data']['features'])\n",
    "                gsu_batch_data['spatial_feat'].append(data['gsu_data']['spatial_feat'])\n",
    "                gsu_batch_data['word2vec'].append(data['gsu_data']['word2vec'])\n",
    "\n",
    "            gsu_batch_data['edge_labels'] = torch.FloatTensor(np.concatenate(gsu_batch_data['edge_labels'], axis=0))\n",
    "            gsu_batch_data['features'] = torch.FloatTensor(np.concatenate(gsu_batch_data['features'], axis=0))\n",
    "            gsu_batch_data['spatial_feat'] = torch.FloatTensor(np.concatenate(gsu_batch_data['spatial_feat'], axis=0))\n",
    "            gsu_batch_data['word2vec'] = torch.FloatTensor(np.concatenate(gsu_batch_data['word2vec'], axis=0))\n",
    "            \n",
    "            cp_batch_data = []\n",
    "            tensors = []\n",
    "            \n",
    "            for data in batch: cp_batch_data.append(data['cp_data'])\n",
    "            if len(self.fields) == 1: cp_batch_data = [cp_batch_data, ]\n",
    "            else: cp_batch_data = list(zip(*cp_batch_data))\n",
    "\n",
    "            for field, data in zip(self.fields.values(), cp_batch_data):\n",
    "                if field == None: tensor = default_collate(data)\n",
    "                else: tensor = field.process(data)\n",
    "                if isinstance(tensor, collections.Sequence) and any(isinstance(t, torch.Tensor) for t in tensor):\n",
    "                    tensors.extend(tensor)\n",
    "                else: tensors.append(tensor)\n",
    "\n",
    "            if len(tensors) > 1:cp_batch_data = tensors\n",
    "            else: cp_batch_data = tensors[0]\n",
    "            \n",
    "            batch_data = {}\n",
    "            batch_data['gsu'] = gsu_batch_data\n",
    "            batch_data['cp'] = cp_batch_data\n",
    "            \n",
    "            return(batch_data)\n",
    "\n",
    "        return collate\n",
    "\n",
    "class PairedDataset(Dataset):\n",
    "    def __init__(self, examples, fields, gsu_const):\n",
    "        assert ('image' in fields)\n",
    "        assert ('text' in fields)\n",
    "        super(PairedDataset, self).__init__(examples, fields, gsu_const)\n",
    "        self.image_field = self.fields['image']\n",
    "        if self.image_field == None: print('no pre-extracted image featured')\n",
    "        self.text_field = self.fields['text']\n",
    "        \n",
    "    def image_dictionary(self, fields=None):\n",
    "        if not fields:\n",
    "            fields = self.fields\n",
    "        dataset = Dataset(self.examples, fields, gsu_const)\n",
    "        return dataset\n",
    "\n",
    "class MTL_DATASET(PairedDataset):\n",
    "    def __init__(self, image_field, text_field, gsu_const, img_root, ann_root, id_root=None):\n",
    "        # setting training and val root\n",
    "        roots = {}\n",
    "        roots['train'] = { 'img': img_root, 'cap': os.path.join(ann_root, 'captions_train.json')}\n",
    "        roots['val'] = {'img': img_root, 'cap': os.path.join(ann_root, 'captions_val.json')}\n",
    "\n",
    "        # Getting the id: planning to remove this in future\n",
    "        if id_root is not None:\n",
    "            ids = {}\n",
    "            ids['train'] = json.load(open(os.path.join(id_root, 'WithCaption_id_path_train.json'), 'r'))\n",
    "            ids['val'] = json.load(open(os.path.join(id_root, 'WithCaption_id_path_val.json'), 'r'))   \n",
    "        else: ids = None\n",
    "        \n",
    "        with nostdout():\n",
    "            self.train_examples, self.val_examples = self.get_samples(roots, ids)\n",
    "        examples = self.train_examples + self.val_examples\n",
    "        super(MTL_DATASET, self).__init__(examples, {'image': image_field, 'text': text_field}, gsu_const)   \n",
    "\n",
    "    @property\n",
    "    def splits(self):\n",
    "        train_split = PairedDataset(self.train_examples, self.fields, gsu_const) \n",
    "        val_split = PairedDataset(self.val_examples, self.fields, gsu_const)\n",
    "        return train_split, val_split\n",
    "\n",
    "    @classmethod\n",
    "    def get_samples(cls, roots, ids_dataset=None):\n",
    "        train_samples = []\n",
    "        val_samples = []\n",
    "   \n",
    "        for split in ['train', 'val']:\n",
    "            anns = json.load(open(roots[split]['cap'], 'r'))\n",
    "            if ids_dataset is not None: ids = ids_dataset[split]\n",
    "                \n",
    "            for index in range(len(ids)):              \n",
    "                id_path = ids[index]\n",
    "                caption = anns[index]['caption']\n",
    "                example = Example.fromdict({'image': os.path.join('', id_path), 'text': caption})\n",
    "                if split == 'train': train_samples.append(example)\n",
    "                elif split == 'val': val_samples.append(example)\n",
    "                    \n",
    "        return train_samples, val_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MTL Model (Graph Scene Understanding and Captioning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mtl_model(nn.Module):\n",
    "    '''\n",
    "    Multi-task model : Graph Scene Understanding and Captioning\n",
    "    Forward uses features from feature_extractor\n",
    "    '''\n",
    "    def __init__(self, feature_extractor, graph, caption):\n",
    "        super(mtl_model, self).__init__()\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.graph_su = graph\n",
    "        self.caption = caption\n",
    "        self.transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
    "    \n",
    "    def forward(self, img_dir, det_boxes_all, caps_gt, node_num, features, spatial_feat, word2vec, roi_labels, val = False, text_field = None):               \n",
    "        \n",
    "        gsu_node_feat = None\n",
    "        cp_node_feat = None\n",
    "\n",
    "        # feature extraction model\n",
    "        for index, img_loc in  enumerate(img_dir):\n",
    "            _img = Image.open(img_loc).convert('RGB')\n",
    "            _img = np.array(_img)\n",
    "            img_stack = None\n",
    "            for idx, bndbox in enumerate(det_boxes_all[index]):        \n",
    "                roi = np.array(bndbox).astype(int)\n",
    "                roi_image = _img[roi[1]:roi[3] + 1, roi[0]:roi[2] + 1, :]\n",
    "                roi_image = self.transform(cv2.resize(roi_image, (224, 224), interpolation=cv2.INTER_LINEAR))\n",
    "                roi_image = torch.autograd.Variable(roi_image.unsqueeze(0))\n",
    "                # stack nodes images per image\n",
    "                if img_stack is None: img_stack = roi_image\n",
    "                else: img_stack = torch.cat((img_stack, roi_image))\n",
    "            \n",
    "            img_stack = img_stack.cuda()\n",
    "            # send the stack to feature extractor\n",
    "            vis_feature = self.feature_extractor(img_stack)\n",
    "            vis_feature = vis_feature.view(vis_feature.size(0), -1)\n",
    "            \n",
    "            if gsu_node_feat == None: gsu_node_feat = vis_feature\n",
    "            else: gsu_node_feat = torch.cat((gsu_node_feat,vis_feature))\n",
    "            \n",
    "            vis_feature = torch.unsqueeze(torch.cat((vis_feature,torch.zeros((6-len(vis_feature)),512).cuda())),0)\n",
    "            if cp_node_feat == None: cp_node_feat = vis_feature\n",
    "            else: cp_node_feat = torch.cat((cp_node_feat,vis_feature))\n",
    "    \n",
    "        # caption model\n",
    "        if val == True: caption_output, _ = self.caption.beam_search(cp_node_feat, 20, text_field.vocab.stoi['<eos>'], 5, out_size=1)\n",
    "        else: caption_output = self.caption(cp_node_feat, caps_gt)\n",
    "        \n",
    "        # graph su model\n",
    "        interaction = self.graph_su(node_num, gsu_node_feat, spatial_feat, word2vec, roi_labels, validation= val)\n",
    "        \n",
    "        return interaction, caption_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(args, text_field, device):\n",
    "    '''\n",
    "    Build MTL model\n",
    "    1) Feature Extraction\n",
    "    2) Caption Model\n",
    "    3) Graph Scene Understanding Model\n",
    "    '''\n",
    "\n",
    "    ''' ==== caption model ===='''\n",
    "    # caption encoder\n",
    "    if args.cp_cbs == 'True':encoder = MemoryAugmentedEncoder_CBS(3, 0, attention_module=ScaledDotProductAttentionMemory, attention_module_kwargs={'m': args.m})\n",
    "    else: encoder = MemoryAugmentedEncoder(3, 0, attention_module=ScaledDotProductAttentionMemory, attention_module_kwargs={'m': args.m}) \n",
    "    # caption decoder\n",
    "    decoder = MeshedDecoder(len(text_field.vocab), 54, 3, text_field.vocab.stoi['<pad>'])\n",
    "    # caption model\n",
    "    caption_model = Transformer(text_field.vocab.stoi['<bos>'], encoder, decoder).to(device)\n",
    "    if args.cp_cbs == 'True': caption_model.encoder.get_new_kernels(0, args.cp_kernel_sizex, args.cp_kernel_sizey, args.cp_decay_epoch, args.cp_std_factor, args.cp_cbs_filter)\n",
    "    # caption load pre-trained weights\n",
    "    pretrained_model = torch.load(args.cp_checkpoint+('%s_best.pth' % args.exp_name))\n",
    "    caption_model.load_state_dict(pretrained_model['state_dict']) \n",
    "\n",
    "    '''==== graph model ===='''\n",
    "    # graph model\n",
    "    graph_su_model = AGRNN(bias= True, bn= False, dropout=0.3, multi_attn=False, layer=1, diff_edge=False, use_cbs = args.gsu_cbs)\n",
    "    if args.gsu_cbs: graph_su_model.grnn1.gnn.apply_h_h_edge.get_new_kernels(0)\n",
    "    # graph load pre-trained weights\n",
    "    pretrained_model = torch.load(args.gsu_checkpoint)\n",
    "    graph_su_model.load_state_dict(pretrained_model['state_dict'])\n",
    "    #graph_su_model.eval()\n",
    "\n",
    "    '''==== Feature extractor ===='''\n",
    "    # feature extraction model\n",
    "    if args.fe_use_SC: feature_network = SupConResNet(args=args)\n",
    "    else: feature_network = ResNet18(args)\n",
    "    if args.fe_use_cbs:\n",
    "        if args.fe_use_SC: feature_network.encoder.get_new_kernels(0)\n",
    "        else: feature_network.get_new_kernels(0)\n",
    "    # based on cuda\n",
    "    num_gpu = torch.cuda.device_count()\n",
    "    if num_gpu > 0:\n",
    "        device_ids = np.arange(num_gpu).tolist()    \n",
    "        if args.fe_use_SC: feature_network.encoder = nn.DataParallel(feature_network.encoder) #feature_network = feature_network.cuda()\n",
    "        else: feature_network = nn.DataParallel(feature_network, device_ids=device_ids)\n",
    "    # feature extraction pre-trained weights\n",
    "    feature_network.load_state_dict(torch.load(args.fe_modelpath))\n",
    "    # extract the encoder layer\n",
    "    if args.fe_use_SC: feature_network = feature_network.encoder\n",
    "    else:\n",
    "        if args.fe_use_cbs: feature_network = nn.Sequential(*list(feature_network.module.children())[:-2])\n",
    "        else: feature_network = nn.Sequential(*list(feature_network.module.children())[:-1])\n",
    "\n",
    "    model = mtl_model(feature_network, graph_su_model, caption_model)\n",
    "    model = model.to(device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_mtl(model, dataloader, text_field):\n",
    "    '''\n",
    "    Evaluate MTL\n",
    "    '''\n",
    "    \n",
    "    gen = {}\n",
    "    gts = {}\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    # graph\n",
    "    g_criterion = nn.MultiLabelSoftMarginLoss()                   \n",
    "    g_edge_count = 0\n",
    "    g_total_acc = 0.0\n",
    "    g_total_loss = 0.0\n",
    "    g_logits_list = []\n",
    "    g_labels_list = []\n",
    "    \n",
    "    for it, data in tqdm(enumerate(iter(dataloader))):\n",
    "            \n",
    "        graph_data = data['gsu']\n",
    "        cp_data = data['cp']\n",
    "            \n",
    "        # graph\n",
    "        #img_name = graph_data['img_name']\n",
    "        #edge_num = graph_data['edge_num']\n",
    "        img_loc = graph_data['img_loc']\n",
    "        node_num = graph_data['node_num']\n",
    "        roi_labels = graph_data['roi_labels']\n",
    "        det_boxes = graph_data['det_boxes']\n",
    "        edge_labels = graph_data['edge_labels']\n",
    "        features = graph_data['features']\n",
    "        spatial_feat = graph_data['spatial_feat']\n",
    "        word2vec = graph_data['word2vec']\n",
    "        features, spatial_feat, word2vec, edge_labels = features.to(device), spatial_feat.to(device), word2vec.to(device), edge_labels.to(device)         \n",
    "        \n",
    "        _, caps_gt = cp_data\n",
    "            \n",
    "        with torch.no_grad():              \n",
    "    \n",
    "            g_output, caption_out = model(img_loc, det_boxes, caps_gt, node_num, features, spatial_feat, word2vec, roi_labels, val = True, text_field = text_field)\n",
    "        \n",
    "            g_logits_list.append(g_output)\n",
    "            g_labels_list.append(edge_labels)\n",
    "            # loss and accuracy\n",
    "            g_loss = g_criterion(g_output, edge_labels.float())\n",
    "            g_acc = np.sum(np.equal(np.argmax(g_output.cpu().data.numpy(), axis=-1), np.argmax(edge_labels.cpu().data.numpy(), axis=-1)))\n",
    "            \n",
    "        # accumulate loss and accuracy of the batch\n",
    "        g_total_loss += g_loss.item() * edge_labels.shape[0]\n",
    "        g_total_acc  += g_acc\n",
    "        g_edge_count += edge_labels.shape[0]\n",
    "        \n",
    "        caps_gen = text_field.decode(caption_out, join_words=False)\n",
    "        \n",
    "        for i, (gts_i, gen_i) in enumerate(zip(caps_gt, caps_gen)):\n",
    "            gen_i = ' '.join([k for k, g in itertools.groupby(gen_i)])\n",
    "            gen['%d_%d' % (it, i)] = [gen_i, ]    \n",
    "            gts['%d_%d' % (it, i)] = [gts_i,]\n",
    "        \n",
    "    #graph evaluation\n",
    "    g_total_acc = g_total_acc / g_edge_count\n",
    "    g_total_loss = g_total_loss / len(dataloader)\n",
    "\n",
    "    g_logits_all = torch.cat(g_logits_list).cuda()\n",
    "    g_labels_all = torch.cat(g_labels_list).cuda()\n",
    "    g_logits_all = F.softmax(g_logits_all, dim=1)\n",
    "    g_map_value, g_ece, g_sce, g_tace, g_brier, g_uce = calibration_metrics(g_logits_all, g_labels_all, 'test')\n",
    "\n",
    "    gts = evaluation.PTBTokenizer.tokenize(gts)\n",
    "    gen = evaluation.PTBTokenizer.tokenize(gen)\n",
    "\n",
    "    scores, _ = evaluation.compute_scores(gts, gen)\n",
    "    print('Graph : {acc: %0.6f map: %0.6f loss: %0.6f, ece:%0.6f, sce:%0.6f, tace:%0.6f, brier:%.6f, uce:%.6f}' %(g_total_acc, g_map_value, g_total_loss, g_ece, g_sce, g_tace, g_brier, g_uce.item()) )\n",
    "    print(print(\"Caption Scores :\", scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train(epoch, lrc, model, dataloader, dict_dataloader_val, text_field):\n",
    "    '''\n",
    "    Finding optimal temperature scale for graph scene understanding task\n",
    "    '''\n",
    "\n",
    "    #if args.optim == 'sgd': \n",
    "    optimizer = optim.SGD(model.feature_extractor.parameters(), lr= lrc, momentum=0.9, weight_decay=0)\n",
    "    #else: optimizer = optim.Adam(model.parameters(), lr= lrc, weight_decay=0)\n",
    "       \n",
    "    g_criterion = nn.MultiLabelSoftMarginLoss()\n",
    "    \n",
    "    for epoch_count in range(epoch):\n",
    "        \n",
    "        model.train()\n",
    "        print(\"=========== Train ===============\")\n",
    "        running_loss = 0.0\n",
    "        running_g_acc = 0.0\n",
    "        running_edge_count = 0\n",
    "        iters = 0\n",
    "        \n",
    "        for it, data in tqdm(enumerate(iter(dataloader))):\n",
    "            iters += 1\n",
    "            \n",
    "            graph_data = data['gsu']\n",
    "            cp_data = data['cp']\n",
    "            \n",
    "            # graph\n",
    "            # img_name = graph_data['img_name']\n",
    "            # edge_num = graph_data['edge_num']\n",
    "            img_loc = graph_data['img_loc']\n",
    "            node_num = graph_data['node_num']\n",
    "            roi_labels = graph_data['roi_labels']\n",
    "            det_boxes = graph_data['det_boxes']\n",
    "            edge_labels = graph_data['edge_labels']\n",
    "            features = graph_data['features']\n",
    "            spatial_feat = graph_data['spatial_feat']\n",
    "            word2vec = graph_data['word2vec']\n",
    "            features, spatial_feat, word2vec, edge_labels = features.to(device), spatial_feat.to(device), word2vec.to(device), edge_labels.to(device)    \n",
    "            \n",
    "            # caption\n",
    "            caption_nodes, caps_gt = cp_data\n",
    "            caption_nodes, caps_gt = caption_nodes.to(device), caps_gt.to(device)\n",
    "            \n",
    "            model.zero_grad()\n",
    "            \n",
    "            interaction, caption_output = model( img_loc, det_boxes, caps_gt, node_num, features, spatial_feat, word2vec, roi_labels, val = False)\n",
    "            \n",
    "            # graph loss and acc\n",
    "            interaction = F.softmax(interaction, dim=1)\n",
    "            g_loss = g_criterion(interaction, edge_labels.float())\n",
    "            g_acc = np.sum(np.equal(np.argmax(interaction.cpu().data.numpy(), axis=-1), np.argmax(edge_labels.cpu().data.numpy(), axis=-1)))\n",
    "                    \n",
    "            # caption loss\n",
    "            c_criterion = CELossWithLS(classes=len(text_field.vocab), smoothing=0.1, gamma=0.0, isCos=False, ignore_index=text_field.vocab.stoi['<pad>'])\n",
    "            c_loss = c_criterion(caption_output[:, :-1].contiguous(), caps_gt[:, 1:].contiguous())\n",
    "            \n",
    "            #uda:\n",
    "            #loss = (0.5 * g_loss) + (0.5 * c_loss)\n",
    "            #uda_graph:\n",
    "            loss = g_loss\n",
    "            #uda_caption:\n",
    "            #loss = c_loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            running_g_acc += g_acc\n",
    "            running_edge_count += edge_labels.shape[0]\n",
    "            #break\n",
    "        epoch_loss = running_loss/float(iters)\n",
    "        epoch_g_acc = running_g_acc/float(running_edge_count)\n",
    "        print(\"[{}] Epoch: {}/{} MTL_Loss: {:0.6f} Graph_Acc: {:0.6f}\".format(\\\n",
    "                            'MTL-Train', epoch_count+1, epoch, epoch_loss, epoch_g_acc))\n",
    "        \n",
    "        checkpoint = {'state_dict': model.state_dict()}\n",
    "        save_name = \"checkpoints/mtl_train/\"+args.mtl_version+\"/checkpoint_\" + str(epoch_count+1) + '_epoch.pth'\n",
    "        torch.save(checkpoint, os.path.join(save_name))\n",
    "        \n",
    "        print(\"=========== Evaluation ===============\")\n",
    "        eval_mtl(model, dict_dataloader_val, text_field)\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arguments, dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1,2\"\n",
    "    device = torch.device('cuda')\n",
    "    # arguments\n",
    "    parser = argparse.ArgumentParser(description='Incremental domain adaptation for surgical report generation')\n",
    "    parser.add_argument('--batch_size',            type=int,       default=8)\n",
    "    parser.add_argument('--workers',               type=int,       default=0)\n",
    "    parser.add_argument('--epoch',                 type=int,       default=20)\n",
    "    # caption\n",
    "    parser.add_argument('--exp_name',              type=str,       default='m2_transformer')\n",
    "    parser.add_argument('--m',                     type=int,       default=40)   \n",
    "    parser.add_argument('--cp_cbs',                type=str,       default='True')\n",
    "    parser.add_argument('--cp_cbs_filter',         type=str,       default='LOG') # Potential choice: 'gau' and 'LOG'\n",
    "    parser.add_argument('--cp_kernel_sizex',       type=int,       default=3)\n",
    "    parser.add_argument('--cp_kernel_sizey',       type=int,       default=1)\n",
    "    parser.add_argument('--cp_decay_epoch',        type=int,       default=2) \n",
    "    parser.add_argument('--cp_std_factor',         type=float,     default=0.9)\n",
    "    # graph\n",
    "    parser.add_argument('--gsu_cbs',               type=bool,      default=True)\n",
    "    parser.add_argument('--gsu_feat',              type=str,       default='resnet18_09_SC_CBS')\n",
    "    parser.add_argument('--gsu_w2v_loc',           type=str,       default='datasets/surgicalscene_word2vec.hdf5')\n",
    "    # feature_extractor\n",
    "    parser.add_argument('--fe_use_cbs',            type=bool,      default=True,        help='use CBS')\n",
    "    parser.add_argument('--fe_std',                type=float,     default=1.0,         help='')\n",
    "    parser.add_argument('--fe_std_factor',         type=float,     default=0.9,         help='')\n",
    "    parser.add_argument('--fe_cbs_epoch',          type=int,       default=5,           help='')\n",
    "    parser.add_argument('--fe_kernel_size',        type=int,       default=3,           help='')\n",
    "    parser.add_argument('--fe_fil1',               type=str,       default='LOG',       help='gau, LOG')\n",
    "    parser.add_argument('--fe_fil2',               type=str,       default='gau',       help='gau, LOG')\n",
    "    parser.add_argument('--fe_fil3',               type=str,       default='gau',       help='gau, LOG')\n",
    "    parser.add_argument('--fe_num_classes',        type=int,       default=11,          help='11')\n",
    "    parser.add_argument('--fe_use_SC',             type=bool,      default=True,        help='use SuperCon')\n",
    "    # SD file dirs\n",
    "    parser.add_argument('--cp_features_path',      type=str,       default='datasets/instruments18/') \n",
    "    parser.add_argument('--cp_annotation_folder',  type=str,       default='datasets/annotations_new/annotations_SD_inc')\n",
    "    parser.add_argument('--gsu_img_dir',           type=str,       default='left_frames')\n",
    "    parser.add_argument('--gsu_file_dir',          type=str,       default='datasets/instruments18/')\n",
    "    # TD file dirs\n",
    "    # parser.add_argument('--cp_features_path',      type=str,       default='datasets/SGH_dataset_2020/') \n",
    "    # parser.add_argument('--cp_annotation_folder',  type=str,       default='datasets/annotations_new/annotations_sgh_inc')\n",
    "    # parser.add_argument('--gsu_img_dir',           type=str,       default='resized_frames')\n",
    "    # parser.add_argument('--gsu_file_dir',          type=str,       default='datasets/SGH_dataset_2020/')\n",
    "    # checkpoints dir\n",
    "    # non-common extractor, MTL, UDA, trained only on SD\n",
    "    parser.add_argument('--cp_checkpoint',         type=str,       default='checkpoints/IDA_MICCAI2021_checkpoints/SD_base_LOG/')\n",
    "    parser.add_argument('--gsu_checkpoint',        type=str,       default='checkpoints/g_checkpoints/da_ecbs_resnet18_09_SC_eCBS/da_ecbs_resnet18_09_SC_eCBS/epoch_train/checkpoint_D1230_epoch.pth')\n",
    "    parser.add_argument('--fe_modelpath',          type=str,       default='feature_extractor/checkpoint/incremental/inc_ResNet18_SC_CBS_0_012345678.pkl')\n",
    "    parser.add_argument('--mtl_version',           type=str,       default='UDA_balanced_loss')\n",
    "    args = parser.parse_args()\n",
    "    print(args)\n",
    "\n",
    "    # graph scene understanding constants\n",
    "    gsu_const = {}\n",
    "    gsu_const['file_dir'] = args.gsu_file_dir\n",
    "    gsu_const['img_dir'] = args.gsu_img_dir\n",
    "    gsu_const['dataconst'] = SurgicalSceneConstants()\n",
    "    gsu_const['feature_extractor'] = args.gsu_feat\n",
    "    gsu_const['w2v_loc'] =args.gsu_w2v_loc\n",
    "\n",
    "\n",
    "    '''==== Dataset ===='''\n",
    "    # Pipeline for image regions and text\n",
    "    image_field = None\n",
    "    #image_field = ImageDetectionsField(detections_path=args.cp_features_path, max_detections=6, load_in_tmp=False)  \n",
    "    text_field = TextField(init_token='<bos>', eos_token='<eos>', lower=True, tokenize='spacy', remove_punctuation=True, nopoints=False)\n",
    "\n",
    "    # Create the dataset \n",
    "    dataset = MTL_DATASET(image_field, text_field, gsu_const, args.cp_features_path, args.cp_annotation_folder, args.cp_annotation_folder)\n",
    "    train_dataset, val_dataset = dataset.splits   \n",
    "    dict_dataset_val = val_dataset.image_dictionary({'image': image_field, 'text': RawField()})\n",
    "    print('train:', len(train_dataset))\n",
    "    print('val:', len(val_dataset))\n",
    "    \n",
    "    # Building vocabulary\n",
    "    if not os.path.isfile('datasets/vocab_%s.pkl' % args.exp_name):\n",
    "        print(\"Building vocabulary\")\n",
    "        text_field.build_vocab(train_dataset, val_dataset, min_freq=2)  \n",
    "        pickle.dump(text_field.vocab, open('datasets/vocab_%s.pkl' % args.exp_name, 'wb'))\n",
    "    else:\n",
    "        text_field.vocab = pickle.load(open('datasets/vocab_%s.pkl' % args.exp_name, 'rb'))\n",
    "\n",
    "    print('vocabulary size is:', len(text_field.vocab))\n",
    "    print(text_field.vocab.stoi)\n",
    "\n",
    "    # data loader\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=False, num_workers=args.workers)\n",
    "    dict_dataloader_val = DataLoader(dict_dataset_val, batch_size=args.batch_size) # for caption with word GT class number\n",
    "\n",
    "    '''==== MTL-model ===='''\n",
    "    model = build_model(args, text_field, device)\n",
    "\n",
    "    '''==== First evaluation MTL ===='''\n",
    "    eval_mtl(model, dict_dataloader_val, text_field)\n",
    "\n",
    "    '''==== MTL-Train model ===='''\n",
    "    # train for 100 epoch\n",
    "    train(args.epoch, 0.001, model, train_dataloader, dict_dataloader_val, text_field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
